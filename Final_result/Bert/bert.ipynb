{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20bb81a-4603-480a-9c59-449a594dfaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e374754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3d4aa2-66e8-46b6-8cad-9c88da2844f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "\n",
    "    for example in data:\n",
    "        example = json.loads(example)\n",
    "\n",
    "        input_text = example['content']\n",
    "        ground_truth = example['groundTruth']\n",
    "        candidates = example['candidates']\n",
    "        if len(input_text) > 450:\n",
    "            continue  \n",
    "        for i, idiom in enumerate(ground_truth):\n",
    "            candidates_str = '，'.join([c for c in candidates[i]])\n",
    "            input_text = input_text.replace('#idiom#', f\"#[MASK][MASK][MASK][MASK]#({candidates_str})\", 1)\n",
    "            # input_text = input_text.replace('#idiom#', f\"#[MASK][MASK][MASK][MASK]#({candidates_str})\", 1)\n",
    "\n",
    "        input_texts.append(input_text)\n",
    "        labels.append(''.join(ground_truth))\n",
    "\n",
    "\n",
    "    concat_inputs = tokenizer(input_texts,return_token_type_ids=False)\n",
    "    labels = tokenizer(labels, return_token_type_ids=False, return_attention_mask=False, add_special_tokens=False)\n",
    "    return concat_inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6735dcd-f623-418d-bb3a-bdf1183c224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdiomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels) -> None:\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.labels = self._get_label(inputs[\"input_ids\"], labels[\"input_ids\"])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "    \n",
    "    def _get_label(self, inputs, labels):\n",
    "        results = []\n",
    "        for inp, label in zip(inputs, labels):\n",
    "            inp = np.array(inp)\n",
    "            l = np.full_like(inp, fill_value=-100)\n",
    "            l[inp == tokenizer.mask_token_id] = label\n",
    "            results.append(l)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"input_ids\": self.inputs[\"input_ids\"][index], \"attention_mask\": self.inputs[\"attention_mask\"][index], \"labels\": self.labels[index]}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input_ids = [torch.LongTensor(each[\"input_ids\"]) for each in batch]\n",
    "    batch_att_mask = [torch.LongTensor(each[\"attention_mask\"]) for each in batch]\n",
    "    batch_label = [torch.LongTensor(each[\"labels\"]) for each in batch]\n",
    "    padded_batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = pad_sequence(batch_att_mask, batch_first=True, padding_value=0)\n",
    "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
    "    \n",
    "def to_device(data, device):\n",
    "    new_data = {}\n",
    "    for k in data:\n",
    "        new_data[k] = data[k].to(device)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17f646e-72cb-42a9-930e-c5f9bc1cdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train_loader:DataLoader, optimizer:optim.Optimizer, log_step=100):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    log_loss = 0.0\n",
    "    for idx, batch in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = to_device(batch, device)\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        log_loss += loss.item()\n",
    "        if idx % log_step == 0:\n",
    "            print(f\"Train Step: {idx} Loss: {log_loss / log_step}\")\n",
    "            log_loss = 0.0\n",
    "    return epoch_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for batch in eval_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        eval_loss += loss.item()\n",
    "        pred = output.logits.argmax(-1)\n",
    "        label = batch[\"labels\"]\n",
    "        correct += torch.where(label!=-100, pred==label, 0).sum().item()\n",
    "        total += torch.sum(label != -100).item()\n",
    "\n",
    "    eval_acc = correct / total\n",
    "    eval_loss = eval_loss / len(eval_loader) \n",
    "    print(total, correct)\n",
    "    return eval_acc, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835f25f7-7e8d-438b-a2eb-f838103a66f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 1.29MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 4.75kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 624/624 [00:00<00:00, 405kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the Chinese Idioms dataset\n",
    "train_data_file = './data/train_20000.txt'\n",
    "val_data_file = './data/dev_3000.txt'\n",
    "\n",
    "\n",
    "with open(train_data_file) as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "with open(val_data_file) as f:\n",
    "    val_data = f.readlines()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\")\n",
    "\n",
    "train_inputs, train_labels = preprocess_data(train_data)\n",
    "val_inputs, val_labels = preprocess_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ac227c-6c24-4ab8-9af4-572a9808c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 深 恶 痛 绝 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 3918 2626 4578 5318 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100]\n",
      "[CLS] 另 据 了 解 ， 北 京 一 个 对 垃 圾 短 信 # [MASK] [MASK] [MASK] [MASK] # ( 深 恶 痛 绝 ， 人 人 自 危 ， 恨 入 骨 髓 ， 不 胜 枚 举 ， 嗤 之 以 鼻 ， 走 马 看 花 ， 不 屑 一 顾 ) 的 老 人 ， 利 用 该 软 件 总 共 呼 死 了 近 2000 个 号 码 。 20 分 钟 呼 上 万 号 码 记 者 昨 天 在 百 度 里 输 入 [UNK] 呼 死 你 软 件 [UNK] ， 出 现 了 7000 多 个 相 关 网 页 ， 随 机 登 录 几 个 网 站 ， 发 现 软 件 均 需 花 钱 购 买 ， 价 格 从 200 元 至 500 元 不 等 。 [SEP]\n",
      "tensor([[ 101, 1369, 2945,  ...,    0,    0,    0],\n",
      "        [ 101, 6821,  671,  ...,    0,    0,    0],\n",
      "        [ 101,  124, 3299,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2190,  754,  ...,    0,    0,    0],\n",
      "        [ 101, 3187, 3144,  ...,    0,    0,    0],\n",
      "        [ 101, 4197, 1400,  ...,    0,    0,    0]])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "val_dataset = IdiomDataset(val_inputs, val_labels)\n",
    "print(tokenizer.decode(val_dataset[0][\"labels\"]))\n",
    "print(val_dataset[0][\"labels\"])\n",
    "print(tokenizer.decode(val_dataset[0][\"input_ids\"]))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "for each in val_loader:\n",
    "    print(each[\"input_ids\"])\n",
    "    print(each.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d709d73-eb41-4156-a496-44580e614721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IdiomDataset(train_inputs, train_labels)\n",
    "val_dataset = IdiomDataset(val_inputs, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b7aae8-b731-4282-82a6-c7249fa7a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 412M/412M [00:14<00:00, 29.4MB/s] \n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n",
      "Train Step: 100 Loss: 2.769947199821472\n",
      "Train Step: 200 Loss: 2.3190110862255096\n",
      "Train Step: 300 Loss: 2.180575000047684\n",
      "Train Step: 400 Loss: 2.027156183719635\n",
      "Train Step: 500 Loss: 1.9760032737255095\n",
      "Train Step: 600 Loss: 1.8805412828922272\n",
      "Train Step: 700 Loss: 1.8137561082839966\n",
      "Train Step: 800 Loss: 1.8058312392234803\n",
      "Train Step: 900 Loss: 1.7075940477848053\n",
      "Train Step: 1000 Loss: 1.7360768568515779\n",
      "Train Step: 1100 Loss: 1.7118392419815063\n",
      "Train Step: 1200 Loss: 1.603399497270584\n",
      "Epoch 1 Training Loss: 1.945467445898056\n",
      "14384 9276\n",
      "Epoch 1 Eval Acc: 0.6448832035595106; Eval Loss: 1.234656422379169\n",
      "Training Epoch 2\n",
      "Train Step: 100 Loss: 1.049015023112297\n",
      "Train Step: 200 Loss: 1.0586186489462852\n",
      "Train Step: 300 Loss: 1.0455415296554564\n",
      "Train Step: 400 Loss: 1.066536609530449\n",
      "Train Step: 500 Loss: 1.0611016768217088\n",
      "Train Step: 600 Loss: 1.039507790505886\n",
      "Train Step: 700 Loss: 1.0176477068662644\n",
      "Train Step: 800 Loss: 0.9722860953211785\n",
      "Train Step: 900 Loss: 1.0262416306138038\n",
      "Train Step: 1000 Loss: 1.0206559067964553\n",
      "Train Step: 1100 Loss: 1.0049728488922118\n",
      "Train Step: 1200 Loss: 1.0389422160387038\n",
      "Epoch 2 Training Loss: 1.0347311388969422\n",
      "14384 9982\n",
      "Epoch 2 Eval Acc: 0.6939655172413793; Eval Loss: 1.1616834575667025\n",
      "Training Epoch 3\n",
      "Train Step: 100 Loss: 0.5650625206530094\n",
      "Train Step: 200 Loss: 0.5373203983902931\n",
      "Train Step: 300 Loss: 0.5822478798776864\n",
      "Train Step: 400 Loss: 0.600940243601799\n",
      "Train Step: 500 Loss: 0.5954488953948021\n",
      "Train Step: 600 Loss: 0.6002684569358826\n",
      "Train Step: 700 Loss: 0.6215791153907776\n",
      "Train Step: 800 Loss: 0.6320340093970299\n",
      "Train Step: 900 Loss: 0.6309121395647526\n",
      "Train Step: 1000 Loss: 0.6276495215296746\n",
      "Train Step: 1100 Loss: 0.6218957522511482\n",
      "Train Step: 1200 Loss: 0.6346806693077087\n",
      "Epoch 3 Training Loss: 0.6051417943537235\n",
      "14384 10204\n",
      "Epoch 3 Eval Acc: 0.7093993325917687; Eval Loss: 1.2254572885626174\n",
      "Training Epoch 4\n",
      "Train Step: 100 Loss: 0.31210589554160834\n",
      "Train Step: 200 Loss: 0.3667385170608759\n",
      "Train Step: 300 Loss: 0.35809090346097944\n",
      "Train Step: 400 Loss: 0.3883620744943619\n",
      "Train Step: 500 Loss: 0.39585063837468626\n",
      "Train Step: 600 Loss: 0.3952441708743572\n",
      "Train Step: 700 Loss: 0.40544378347694876\n",
      "Train Step: 800 Loss: 0.3717859746515751\n",
      "Train Step: 900 Loss: 0.3991606868058443\n",
      "Train Step: 1000 Loss: 0.4016060408949852\n",
      "Train Step: 1100 Loss: 0.4107267874479294\n",
      "Train Step: 1200 Loss: 0.4015993355214596\n",
      "Epoch 4 Training Loss: 0.38412484527528284\n",
      "14384 10211\n",
      "Epoch 4 Eval Acc: 0.7098859844271412; Eval Loss: 1.4938684215253972\n",
      "Training Epoch 5\n",
      "Train Step: 100 Loss: 0.19127465112134814\n",
      "Train Step: 200 Loss: 0.2331966731324792\n",
      "Train Step: 300 Loss: 0.24592815731652082\n",
      "Train Step: 400 Loss: 0.25268038289621475\n",
      "Train Step: 500 Loss: 0.22686354985460638\n",
      "Train Step: 600 Loss: 0.26251089461147786\n",
      "Train Step: 700 Loss: 0.2685901843942702\n",
      "Train Step: 800 Loss: 0.2597976635210216\n",
      "Train Step: 900 Loss: 0.28954952040687204\n",
      "Train Step: 1000 Loss: 0.26006287820637225\n",
      "Train Step: 1100 Loss: 0.298042958676815\n",
      "Train Step: 1200 Loss: 0.2984005685523152\n",
      "Epoch 5 Training Loss: 0.25731067710891364\n",
      "14384 10229\n",
      "Epoch 5 Eval Acc: 0.7111373748609566; Eval Loss: 1.521704657082545\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\").to(device)\n",
    "\n",
    "epoches = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, epoches+1):\n",
    "    print(f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch} Training Loss: {train_loss}\")\n",
    "    \n",
    "    eval_acc, eval_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n",
    "torch.save(model.state_dict(), \"bert_ckpt_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9f96111-1454-4ffd-9f3d-de953278bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_idiom(model, loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for batch in loader:   \n",
    "        batch = to_device(batch, device)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = output.logits.argmax(-1)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            idiom_idx = label != -100\n",
    "            l = label[idiom_idx].split(4)\n",
    "            p = pred[idiom_idx].split(4)\n",
    "            gold_idiom =  set(\"\".join(tokenizer.convert_ids_to_tokens(i)) for i in l)\n",
    "            pred_idiom = set(\"\".join(tokenizer.convert_ids_to_tokens(i)) for i in p)\n",
    "            all_labels.append(set(gold_idiom))\n",
    "            all_preds.append(set(pred_idiom))\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "        \n",
    "\n",
    "def f1_score(sys, gold):\n",
    "    tp = 0\n",
    "    t = 0\n",
    "    p = 0\n",
    "    for s, g in zip(sys, gold):\n",
    "        t += len(g)\n",
    "        p += len(s)\n",
    "        tp += len(g & s)\n",
    "    precision = tp / p if p != 0 else 0\n",
    "    recall = tp / t if t != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return precision, recall, f1, tp, t, p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e254f55-b16a-4ef8-8c65-09bac7978272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\").to(device)\n",
    "model.load_state_dict(torch.load(\"bert_ckpt_new.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b934ffd-6b88-4580-8137-23690a0719ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t and p score for Test set is 3581,3583\n",
      "F1 score for Test set is 0.579123639408317,0.5794470818207205,0.57928531546622\n",
      "Accuracy for Test set is 2075\n"
     ]
    }
   ],
   "source": [
    "sys, gold = fill_idiom(model, val_loader)\n",
    "pre, r, f1, tp, t, p = f1_score(sys, gold)\n",
    "\n",
    "print((f\"t and p score for Test set is {t},{p}\"))\n",
    "print((f\"F1 score for Test set is {pre},{r},{f1}\"))\n",
    "print(f\"Accuracy for Test set is {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "860a5aa0-702d-48f1-86de-9db6b9c5b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chinese Idioms dataset For Test set\n",
    "test_data_file = 'data/test_3000.txt'\n",
    "\n",
    "with open(test_data_file, encoding='utf-8', errors='ignore') as f:\n",
    "    test_data = f.readlines()\n",
    "\n",
    "test_inputs, test_labels = preprocess_data(test_data)\n",
    "test_dataset = IdiomDataset(test_inputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b73ef4-8f16-4654-a451-2ac7b45e71fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t and p score for Test set is 3517,3517\n",
      "F1 score for Test set is 0.5794711401762866,0.5794711401762866,0.5794711401762866\n",
      "Accuracy for Test set is 2038\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "model.load_state_dict(torch.load(\"bert_ckpt_new.pt\", map_location=device))\n",
    "sys, gold = fill_idiom(model, test_loader)\n",
    "pre, r, f1, tp, t, p = f1_score(sys, gold)\n",
    "\n",
    "print((f\"t and p score for Test set is {t},{p}\"))\n",
    "print((f\"F1 score for Test set is {pre},{r},{f1}\"))\n",
    "print(f\"Accuracy for Test set is {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b22e28d-903e-41ea-bfe9-39480d44f3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
