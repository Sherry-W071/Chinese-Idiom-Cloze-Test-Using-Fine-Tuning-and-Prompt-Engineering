{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "931cb5cd-10af-445e-b38c-389bb5d8837d",
      "metadata": {
        "id": "931cb5cd-10af-445e-b38c-389bb5d8837d"
      },
      "source": [
        "# T5/mT5-small: Prompt Engineering, Evaluation and Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338f1500-424a-44af-991a-ab96caf01238",
      "metadata": {
        "id": "338f1500-424a-44af-991a-ab96caf01238"
      },
      "source": [
        "## Imports and Device Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2db85f5a-7e83-4f2e-9e5b-783132661495",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2db85f5a-7e83-4f2e-9e5b-783132661495",
        "outputId": "6bc2398b-4566-49de-b4f4-e04964dbda78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.98)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install sentencepiece    # for AutoTokenizer\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "from torch import cuda, nn, optim\n",
        "from transformers import BertTokenizer, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import TrainingArguments, Trainer, logging\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11dcc013-acec-4506-b1a4-a17912c5d13f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11dcc013-acec-4506-b1a4-a17912c5d13f",
        "outputId": "97784d2e-277a-43d2-b76c-700112b9a26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "manual_seed = 585\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a0aa74f8-a3fa-4746-a720-c831ac6ef645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0aa74f8-a3fa-4746-a720-c831ac6ef645",
        "outputId": "cffa88a2-a009-4d9c-da33-41c3a971d013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'groundTruth': ['现身说法'], 'candidates': [['旷日持久', '公正廉洁', '苦口婆心', '现身说法', '白日做梦', '深入浅出', '肺腑之言']], 'content': '只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则#idiom#，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。', 'realCount': 1}, {'groundTruth': ['神来之笔', '赞不绝口'], 'candidates': [['画龙点睛', '悔过自新', '拍案叫绝', '鬼斧神工', '神来之笔', '颠倒黑白', '中流砥柱'], ['敬谢不敏', '拍案叫绝', '心悦诚服', '叹为观止', '赞不绝口', '口口声声', '扬眉吐气']], 'content': '亨利的这个#idiom#被法国媒体形容为“空中舞蹈”，亨利自己对球队表现也很满意，“上半场开局一般，但很快觉醒，下半场的进攻让人看到真正的法国，尤其是我们的速度让对方有了麻烦。”而和亨利搭档的本泽马对老大哥#idiom#，“和他在一起配合很容易，他给了我很多信心。”', 'realCount': 2}] \n",
            " 3000\n"
          ]
        }
      ],
      "source": [
        "# path = '../data/'    # change the path as needed\n",
        "path = '/content/gdrive/My Drive/585data/'\n",
        "\n",
        "def read_data(file):\n",
        "    with open (path+file) as t:\n",
        "        data = t.readlines()\n",
        "        for i in range(len(data)):\n",
        "            data[i] = eval(data[i])\n",
        "    return data\n",
        "\n",
        "test_set = read_data('test_data.txt')[:3000]\n",
        "\n",
        "# type(train_set)\n",
        "print(test_set[:2], '\\n', len(test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Functions"
      ],
      "metadata": {
        "id": "eE3r5BY-pVAo"
      },
      "id": "eE3r5BY-pVAo"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "TvhKNUaaOh3j",
      "metadata": {
        "id": "TvhKNUaaOh3j"
      },
      "outputs": [],
      "source": [
        "def f1_score(sys, gold):\n",
        "    tp = 0\n",
        "    total = 0\n",
        "    pos = 0\n",
        "    for s, g in zip(sys, gold):\n",
        "        total += len(g)\n",
        "        pos += len(s)\n",
        "        tp += len(g & s)\n",
        "    precision = tp / pos if pos != 0 else 0\n",
        "    recall = tp / total if total != 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "    return precision, recall, f1, tp\n",
        "    \n",
        "def accuracy(sys, gold, tp):\n",
        "    total = 0\n",
        "    for s, g in zip(sys, gold):\n",
        "        total += len(g)\n",
        "    return tp / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c2f97e-7c04-4abc-b23a-8e4a29ec7ffc",
      "metadata": {
        "id": "f7c2f97e-7c04-4abc-b23a-8e4a29ec7ffc"
      },
      "source": [
        "## Load the Pre-trained T5-small Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "99ae48ef-34a0-44fd-8489-0d4b13dc73d0",
      "metadata": {
        "id": "99ae48ef-34a0-44fd-8489-0d4b13dc73d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4762d7-4286-471e-ab70-772f5b94216a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(21228, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=21228, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "base_t5_cn = T5ForConditionalGeneration.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
        "base_t5_cn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizers\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSryEB9D1J2_",
        "outputId": "50e642ac-aaa0-4831-aba7-e04fe1989477"
      },
      "id": "HSryEB9D1J2_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad73c3b-4464-432e-944d-2f9ce514e7ac",
      "metadata": {
        "id": "8ad73c3b-4464-432e-944d-2f9ce514e7ac"
      },
      "source": [
        "## Can Prompt Engineering Only Work Well?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "886d1d6b-1517-4a1a-8f42-e5059e0802f5",
      "metadata": {
        "id": "886d1d6b-1517-4a1a-8f42-e5059e0802f5"
      },
      "outputs": [],
      "source": [
        "def prompt(data):\n",
        "    '''data shall be the output of `read_data`'''\n",
        "    text_input = []\n",
        "    gold_text = []\n",
        "    for i in range(len(data)):\n",
        "        # data[i] = eval(data[i])\n",
        "        input_text = data[i]['content']\n",
        "        candidates = data[i]['candidates']\n",
        "        ground_truth = set(data[i]['groundTruth'])\n",
        "        gold_text.append(ground_truth)\n",
        "\n",
        "        candidate_str = ''\n",
        "        for candidate in candidates:\n",
        "            candidate_str += '('+'|'.join(candidate)+')'\n",
        "        \n",
        "        preprocess_idx = -1\n",
        "        def replace(match):\n",
        "            nonlocal preprocess_idx\n",
        "            preprocess_idx += 1\n",
        "            return 'extra{}'.format(preprocess_idx)\n",
        "        input_text = re.sub(r'#idiom#', replace, input_text)\n",
        "\n",
        "        instruction = '请从下列括号中分别选择合适的成语填入空缺处：{}'.format(candidate_str)\n",
        "        \n",
        "        text_input.append(instruction+'\\n'+input_text)\n",
        "\n",
        "    return text_input, gold_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bd9d7eb9-9e2c-4c49-b041-df5e3eeaeaab",
      "metadata": {
        "id": "bd9d7eb9-9e2c-4c49-b041-df5e3eeaeaab"
      },
      "outputs": [],
      "source": [
        "def postprocess(text):\n",
        "    text = text.replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\").replace(\"8\", \"\").replace(\"9\", \"\")\n",
        "    return text.replace(\".\", \"\").replace(' ','').replace('extra', ',')\n",
        "    \n",
        "def answer_fn(model, text, tokenizer, top_k=50):\n",
        "    encoding = tokenizer(text=[text], truncation=True, padding=True, max_length=256, return_tensors=\"pt\", return_token_type_ids=False).to(device) \n",
        "    out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_length=512,temperature=0.5,do_sample=True,repetition_penalty=3.0 ,top_k=top_k)\n",
        "    result = tokenizer.batch_decode(out[\"sequences\"], skip_special_tokens=True)\n",
        "    return postprocess(result[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-small Pretrained on Chinese\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MsB9ygQjHc7d"
      },
      "id": "MsB9ygQjHc7d"
    },
    {
      "cell_type": "code",
      "source": [
        "# these codes run super slow, and the example shows a mess, so we only test on a small set\n",
        "test_input, gold_text = prompt(test_set[:100])\n",
        "print(test_input[0])\n",
        "answer_fn(base_t5_cn, test_input[0], bert_tokenizer)"
      ],
      "metadata": {
        "id": "8TTRuQdiK2jQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c714742e-6fff-4b00-8a35-b241cc6f5399"
      },
      "id": "8TTRuQdiK2jQ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请从下列括号中分别选择合适的成语填入空缺处：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'但开,编尽管,先,虽,本经精紧定,一才,西。无字,,于同,果新国身与,金段,至,奇,到,上,吃,、得不知,：免,美,守等,单基网世,有,子文便,值意限心科,班小学月,会,,点，,何格,修（论自分方？位着,送的业下作,,,额马你更社神从旧,前,过问情付和,批,微博终气委发家生总低代,手,华命性为之变成,,,外余热折时卡,指均,我中居,要进,是,官险推真以寸,看则说老最费,,三,股莫主刘传恩,号通梦否,能人工图,,,非初,笔,日全面收效,周显易错实环负内来往黄,,,可奥,后水,二(,计向克口究程银英女物各在其,保正,尚然,盘除将,万准假,年,男！深含左胡难右重称,供虚权波,关底,多表记北出,省既,,品名,,,还双,起,立隐四被元东千,交听,,,依,事西型城两晚茶据古,,,他,,,高用坐注早【,《,镇著,特,固,空了,,,,,西化奖现长群而雅?证,压,这种,必止活尽际常把,明没菜者新零,,别,信,确道,,好米皮筋动,,楼,,专,因,达欢,喜市,吉言客村c尽此尽,农户部整处约张,喝菲懂又爱,离,印,就夫样,打搞端贵致价,员取,,汤,,码路,综南尽,,台,,套源,死,字,简,近到,不,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gold_text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkGoqE8uoPza",
        "outputId": "7aab8f5e-f6ab-4641-c55a-7ff5cbb3be73"
      },
      "id": "GkGoqE8uoPza",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'现身说法'},\n",
              " {'神来之笔', '赞不绝口'},\n",
              " {'难分难舍'},\n",
              " {'先天不足'},\n",
              " {'不寒而栗'},\n",
              " {'凶多吉少'},\n",
              " {'堂而皇之'},\n",
              " {'精神抖擞'},\n",
              " {'孤注一掷'},\n",
              " {'闭门造车'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys = []\n",
        "for q in test_input:  \n",
        "    a = answer_fn(base_t5_cn, q, bert_tokenizer)\n",
        "    a = a.strip(' .。，#<extra0123456789>').split(',')\n",
        "    sys.append(set(a))"
      ],
      "metadata": {
        "id": "kj-aGTV0M7I7"
      },
      "id": "kj-aGTV0M7I7",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1, tp = f1_score(sys, gold_text)\n",
        "print('Prompt Engineering on T5-small before fine-tuning: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold_text, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT10D202Rb9o",
        "outputId": "71c81e90-20b0-460c-8606-6832ddd047fb"
      },
      "id": "gT10D202Rb9o",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Engineering on T5-small before fine-tuning: \n",
            "Accuracy for test set is 0.0\n",
            "F1 score for test set is 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mT5-small\n"
      ],
      "metadata": {
        "id": "rFPsoP4-Hw10"
      },
      "id": "rFPsoP4-Hw10"
    },
    {
      "cell_type": "code",
      "source": [
        "base_mt5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
        "base_mt5.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlkf_V32Gxbx",
        "outputId": "f526ef38-c60f-4066-eceb-81665b2d4de1"
      },
      "id": "Rlkf_V32Gxbx",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MT5ForConditionalGeneration(\n",
              "  (shared): Embedding(250112, 512)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_input[0])\n",
        "answer_fn(base_mt5, test_input[0], auto_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "rQAs3-4WG3Yw",
        "outputId": "4c6ed1e3-f003-421d-df63-6311a1f648cd"
      },
      "id": "rQAs3-4WG3Yw",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请从下列括号中分别选择合适的成语填入空缺处：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<,_id_>。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys = []\n",
        "for q in test_input:  \n",
        "    a = answer_fn(base_t5_cn, q, bert_tokenizer)\n",
        "    a = a.replace('_id_', '').strip('<>. ').split(',')\n",
        "    sys.append(set(a))"
      ],
      "metadata": {
        "id": "FYwNH_KhHJC-"
      },
      "id": "FYwNH_KhHJC-",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1, tp = f1_score(sys, gold_text)\n",
        "print('Prompt Engineering on mT5-small before fine-tuning: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold_text, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ig0O7gHLEB",
        "outputId": "e77f4e88-2db9-461b-decf-060b73f6ce6e"
      },
      "id": "7-ig0O7gHLEB",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Engineering on mT5-small before fine-tuning: \n",
            "Accuracy for test set is 0.0\n",
            "F1 score for test set is 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfvrjzqPI3t5",
        "outputId": "41f7f805-5f3f-4ae9-cb24-e01459920765"
      },
      "id": "pfvrjzqPI3t5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'',\n",
              "  '(四时城压了男张',\n",
              "  '?复',\n",
              "  '。反',\n",
              "  '一',\n",
              "  '下无',\n",
              "  '不',\n",
              "  '与点和基段',\n",
              "  '专',\n",
              "  '东信新盘合李空袁白',\n",
              "  '为称多',\n",
              "  '主权约中',\n",
              "  '之余免知科性',\n",
              "  '书',\n",
              "  '事含程物周代',\n",
              "  '二同寸',\n",
              "  '亲言高处既',\n",
              "  '以图班于子上值西',\n",
              "  '传他管但水离正交积实女费',\n",
              "  '位论',\n",
              "  '何',\n",
              "  '依',\n",
              "  '修错通',\n",
              "  '先准微奇将',\n",
              "  '其',\n",
              "  '写双欢负楼活',\n",
              "  '分月格是',\n",
              "  '创',\n",
              "  '北致',\n",
              "  '单',\n",
              "  '印码神非',\n",
              "  '发。',\n",
              "  '古省克向晚',\n",
              "  '可批开计得',\n",
              "  '史假头',\n",
              "  '右',\n",
              "  '名英',\n",
              "  '后',\n",
              "  '听《',\n",
              "  '品',\n",
              "  '因深',\n",
              "  '固尽千',\n",
              "  '国你到从限学',\n",
              "  '均菜市立',\n",
              "  '外口然号种付供梦三端起源股除两',\n",
              "  '好记账',\n",
              "  '委小',\n",
              "  '字',\n",
              "  '守',\n",
              "  '尚',\n",
              "  '工',\n",
              "  '已达零搞出',\n",
              "  '年',\n",
              "  '往用奥',\n",
              "  '心',\n",
              "  '息',\n",
              "  '成过低世定人',\n",
              "  '我',\n",
              "  '房户西',\n",
              "  '打贵',\n",
              "  '技重奖文倒装么',\n",
              "  '把',\n",
              "  '折',\n",
              "  '数著',\n",
              "  '新金会',\n",
              "  '日网问',\n",
              "  '旧自方',\n",
              "  '更',\n",
              "  '有收终',\n",
              "  '本尽明才',\n",
              "  '机',\n",
              "  '来说卡',\n",
              "  '果马文经吃否生',\n",
              "  '样被全环家',\n",
              "  '死',\n",
              "  '气真胡着',\n",
              "  '汉价',\n",
              "  '爱懂动',\n",
              "  '特',\n",
              "  '现',\n",
              "  '疑额作易莫指最初前皮',\n",
              "  '的',\n",
              "  '看华老官证',\n",
              "  '社推进手、',\n",
              "  '笔',\n",
              "  '米夫',\n",
              "  '精',\n",
              "  '紧总居万',\n",
              "  '美',\n",
              "  '而效',\n",
              "  '至',\n",
              "  '菲新',\n",
              "  '藏命显究西西欧波型银',\n",
              "  '虚隐',\n",
              "  '要底',\n",
              "  '身',\n",
              "  '轻刘恩别左长注',\n",
              "  '送变意，能等便',\n",
              "  '酷劲化',\n",
              "  '险',\n",
              "  '面关保各博者常短',\n",
              "  '题客据',\n",
              "  '黄坐在则还早',\n",
              "  '！',\n",
              "  '（',\n",
              "  '）',\n",
              "  '：情热业内',\n",
              "  '？'},\n",
              " {'',\n",
              "  '(科世',\n",
              "  '、身莫：（三寸',\n",
              "  '万之',\n",
              "  '上个段',\n",
              "  '业',\n",
              "  '交批初计',\n",
              "  '亲【',\n",
              "  '他热作',\n",
              "  '以立北多现还刘记居命男均',\n",
              "  '但笔两口究神',\n",
              "  '位便',\n",
              "  '低华微旧心',\n",
              "  '何网精',\n",
              "  '你生限',\n",
              "  '克',\n",
              "  '关到深早',\n",
              "  '内积',\n",
              "  '准',\n",
              "  '分和',\n",
              "  '到知',\n",
              "  '动',\n",
              "  '单与',\n",
              "  '压把付钱',\n",
              "  '双不',\n",
              "  '发过',\n",
              "  '古',\n",
              "  '只西元到',\n",
              "  '右型夫含底',\n",
              "  '号险要',\n",
              "  '吃',\n",
              "  '各环班',\n",
              "  '名',\n",
              "  '向难',\n",
              "  '否',\n",
              "  '听好',\n",
              "  '告',\n",
              "  '喜既彭近隐化却',\n",
              "  '固东',\n",
              "  '复',\n",
              "  '外梦易左真其问时',\n",
              "  '奇下国',\n",
              "  '字紧方从一西',\n",
              "  '守成',\n",
              "  '定同',\n",
              "  '小',\n",
              "  '就此友不',\n",
              "  '尽',\n",
              "  '尽值',\n",
              "  '已',\n",
              "  '常吉',\n",
              "  '往二重',\n",
              "  '得家子可，',\n",
              "  '性气将代？开免委送',\n",
              "  '总额',\n",
              "  '情',\n",
              "  '懂省四西',\n",
              "  '我',\n",
              "  '打',\n",
              "  '技搞弄西',\n",
              "  '据。程称工全',\n",
              "  '效官学',\n",
              "  '新',\n",
              "  '新贵',\n",
              "  '无折管文基卡修',\n",
              "  '日图非',\n",
              "  '是来',\n",
              "  '更',\n",
              "  '月',\n",
              "  '有着余',\n",
              "  '本周终',\n",
              "  '机',\n",
              "  '样菜女南汉农',\n",
              "  '格则奥保',\n",
              "  '欧合巧不仅路数必',\n",
              "  '正处收银丝显明股市集',\n",
              "  '水到',\n",
              "  '波后',\n",
              "  '活毛价',\n",
              "  '消西',\n",
              "  '点',\n",
              "  '然源零头',\n",
              "  '物',\n",
              "  '用起城客',\n",
              "  '白',\n",
              "  '的',\n",
              "  '码',\n",
              "  '离',\n",
              "  '端',\n",
              "  '笑权西',\n",
              "  '等最',\n",
              "  '米',\n",
              "  '约',\n",
              "  '经意',\n",
              "  '美',\n",
              "  '自于',\n",
              "  '至推',\n",
              "  '荐',\n",
              "  '虚而又',\n",
              "  '虽在',\n",
              "  '表',\n",
              "  '被',\n",
              "  '观',\n",
              "  '论',\n",
              "  '证果前老变黄人',\n",
              "  '费',\n",
              "  '这才',\n",
              "  '进',\n",
              "  '通专供品空镇年菲晚',\n",
              "  '酷',\n",
              "  '金',\n",
              "  '错恩博指',\n",
              "  '长',\n",
              "  '际',\n",
              "  '除英手为皮先马能主会看中传实盘负胡社说',\n",
              "  '面奖茶注',\n",
              "  '高确',\n",
              "  '！'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf1f4b3-2f98-4704-bff4-12f1f9ad055a",
      "metadata": {
        "id": "2bf1f4b3-2f98-4704-bff4-12f1f9ad055a"
      },
      "source": [
        "## Preprocess Test Input to Feed in the Fine-tuned Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9c3965b6-eaff-42ff-ac70-dac21b8dbd21",
      "metadata": {
        "id": "9c3965b6-eaff-42ff-ac70-dac21b8dbd21"
      },
      "outputs": [],
      "source": [
        "def preprocess(data, tokenizer):\n",
        "    text_input = []\n",
        "    idiom_output = []\n",
        "    for i in range(len(data)):\n",
        "        input_text = data[i]['content']\n",
        "        ground_truth = data[i]['groundTruth']\n",
        "        candidates = data[i]['candidates']\n",
        "\n",
        "        candidate_str = ''\n",
        "        for candidate in candidates:\n",
        "            candidate_str += '('+'|'.join(candidate)+')'\n",
        "        \n",
        "        preprocess_idx = -1\n",
        "        def replace(match):\n",
        "            nonlocal preprocess_idx\n",
        "            preprocess_idx += 1\n",
        "            return 'extra{}'.format(preprocess_idx)\n",
        "        input_text = re.sub(r'#idiom#', replace, input_text)\n",
        "\n",
        "        instruction = '请从下列括号中分别选择合适的成语填入空缺处：{}'.format(candidate_str)\n",
        "        # input_text = input_text.replace('#idiom#', '_')\n",
        "        output_text = ','.join(ground_truth)\n",
        "        \n",
        "        text_input.append(instruction+'\\n'+input_text)\n",
        "        idiom_output.append(output_text)\n",
        "    \n",
        "    print(text_input[0], idiom_output[0])    \n",
        "    input_tok = tokenizer.batch_encode_plus(text_input,\n",
        "                                            add_special_tokens=False, \n",
        "                                            return_token_type_ids=False)\n",
        "    output_tok = tokenizer.batch_encode_plus(idiom_output, \n",
        "                                             add_special_tokens=False,\n",
        "                                             return_token_type_ids=False)\n",
        "    return input_tok, output_tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "745d083f-5ef4-4ef2-95ae-88edc375a5aa",
      "metadata": {
        "id": "745d083f-5ef4-4ef2-95ae-88edc375a5aa"
      },
      "outputs": [],
      "source": [
        "class IdiomDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs['input_ids'][idx]\n",
        "        attention_mask = self.inputs['attention_mask'][idx]\n",
        "\n",
        "        target_ids = self.outputs['input_ids'][idx]\n",
        "        target_attention_mask = self.outputs['attention_mask'][idx]\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\":attention_mask, \"output_ids\":target_ids}\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_input = [torch.LongTensor(example['input_ids']) for example in batch]\n",
        "    batch_output = [torch.LongTensor(example['output_ids']) for example in batch]\n",
        "    batch_mask = [torch.LongTensor(example['attention_mask']) for example in batch]\n",
        "\n",
        "    padded_batch_input_ids = pad_sequence(batch_input, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    padded_batch_label = pad_sequence(batch_output, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    padded_batch_att_mask = pad_sequence(batch_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
        "\n",
        "def to_device(data, device):\n",
        "    new_data = {}\n",
        "    for k in data:\n",
        "        # k = k.to(device)\n",
        "        new_data[k] = data[k].to(device)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "DLFW9KkpjPQ9",
      "metadata": {
        "id": "DLFW9KkpjPQ9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def fill_idiom(model, loader, tokenizer):\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch = to_device(batch, device)\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        outputs = model.generate(input_ids=input_ids, \n",
        "                                 attention_mask=attention_mask, \n",
        "                                 return_dict_in_generate=True, \n",
        "                                 pad_token_id=tokenizer.pad_token_id, \n",
        "                                 max_length=512, \n",
        "                                 top_k=15)\n",
        "        truncated_outputs = []\n",
        "\n",
        "        decode_texts = tokenizer.batch_decode([l[l != 0] for l in outputs['sequences']])\n",
        "        gold_texts = tokenizer.batch_decode([l[l != 0] for l in labels])\n",
        "        # print(decode_texts, gold_texts)\n",
        "        for gold, decode in zip(gold_texts, decode_texts):\n",
        "            l = set(gold.replace(' ', '').replace('[CLS]', '').split(','))\n",
        "            p = set(decode.replace(' ', '').replace('[CLS]', '').split(','))\n",
        "            # print(l, p)\n",
        "            all_labels.append(l)\n",
        "            all_preds.append(p)\n",
        "        # print(decode_texts)\n",
        "        # print(gold_texts)\n",
        "        # break\n",
        "    \n",
        "    return all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Fine-tuned Models on Test Set"
      ],
      "metadata": {
        "id": "26NEG1WctMH4"
      },
      "id": "26NEG1WctMH4"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9ad9389a-0f2a-4222-8f67-e9f968e6c734",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ad9389a-0f2a-4222-8f67-e9f968e6c734",
        "outputId": "42e9ff3a-fe4d-4d1a-c9c1-33c5a0a557d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请从下列括号中分别选择合适的成语填入空缺处：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。 现身说法\n"
          ]
        }
      ],
      "source": [
        "# for T5-small\n",
        "test_input, test_output = preprocess(test_set, bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2306fb26-febb-4159-b957-89944b73a9c4",
      "metadata": {
        "id": "2306fb26-febb-4159-b957-89944b73a9c4"
      },
      "outputs": [],
      "source": [
        "test_dataset = IdiomDataset(test_input, test_output)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-small Fine-tuned on the 20,000 Training Data"
      ],
      "metadata": {
        "id": "0K28NVYDuAym"
      },
      "id": "0K28NVYDuAym"
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_t5_cn = T5ForConditionalGeneration.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
        "tuned_t5_cn.load_state_dict(torch.load(path+\"T5-small_model_5epoch.pt\", map_location=device))\n",
        "tuned_t5_cn.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMkQ2XxYkENl",
        "outputId": "ccc77b20-b804-4ed9-fd7b-23f247cd6b18"
      },
      "id": "qMkQ2XxYkENl",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(21228, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=21228, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ZiMhMM_ZjWsj",
      "metadata": {
        "id": "ZiMhMM_ZjWsj"
      },
      "outputs": [],
      "source": [
        "tokenizer = bert_tokenizer   # for colate_fn function\n",
        "sys, gold = fill_idiom(tuned_t5_cn, test_loader, bert_tokenizer)\n",
        "p, r, f1, tp = f1_score(sys, gold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "NVeTyoI4MDRF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVeTyoI4MDRF",
        "outputId": "af53d4bd-7888-4095-804c-54bc041b0d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5-small trained on 20000 data: \n",
            "Accuracy for test set is 0.4138309549945115\n",
            "F1 score for test set is 0.4152554041029877\n"
          ]
        }
      ],
      "source": [
        "print('T5-small trained on 20000 data: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5-small Fine-tuned on 200,000 Training Data (NOT Standard)"
      ],
      "metadata": {
        "id": "xoSixnwduWJw"
      },
      "id": "xoSixnwduWJw"
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_t5_cn_bd = T5ForConditionalGeneration.from_pretrained(\"uer/t5-small-chinese-cluecorpussmall\")\n",
        "tuned_t5_cn_bd.load_state_dict(torch.load(path+\"T5-small_model_3epoch_20wData.pt\", map_location=device))\n",
        "tuned_t5_cn_bd.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N75_eW4dkG73",
        "outputId": "71f5fc54-de6e-48ae-dea5-8fae2fcb4457"
      },
      "id": "N75_eW4dkG73",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(21228, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(21228, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=21228, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys, gold = fill_idiom(tuned_t5_cn_bd, test_loader, bert_tokenizer)\n",
        "p, r, f1, tp = f1_score(sys, gold)"
      ],
      "metadata": {
        "id": "MRVuC5_6KOAA"
      },
      "id": "MRVuC5_6KOAA",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('T5-small trained on 200000 data: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "id": "KlqFmEg_KRZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84c96cc-7498-421d-a703-835e4924cafb"
      },
      "id": "KlqFmEg_KRZk",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5-small trained on 200000 data: \n",
            "Accuracy for test set is 0.6789242590559824\n",
            "F1 score for test set is 0.6791106231128191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mT5-small Fine-tuned on 20,000 Training Data"
      ],
      "metadata": {
        "id": "kJHecI3qufzN"
      },
      "id": "kJHecI3qufzN"
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_mt5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
        "tuned_mt5.load_state_dict(torch.load(path+\"mT5-small_model_5epoches.pt\", map_location=device))\n",
        "tuned_mt5.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enFEo9ACkO9o",
        "outputId": "0cae019d-c3dc-45e0-dba9-33d8d773fc93"
      },
      "id": "enFEo9ACkO9o",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MT5ForConditionalGeneration(\n",
              "  (shared): Embedding(250112, 512)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The tokenizer is changed, so the test data needs to be re-preprocessed\n",
        "test_input, test_output = preprocess(test_set, auto_tokenizer)\n",
        "test_dataset = IdiomDataset(test_input, test_output)\n",
        " \n",
        "tokenizer = auto_tokenizer  # for collate_fn function\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yR5oNJjlqDr",
        "outputId": "2d9d9892-552b-486b-8669-c20ddd6613fa"
      },
      "id": "-yR5oNJjlqDr",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请从下列括号中分别选择合适的成语填入空缺处：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。 现身说法\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys, gold = fill_idiom(tuned_mt5, test_loader, auto_tokenizer)\n",
        "p, r, f1, tp = f1_score(sys, gold)"
      ],
      "metadata": {
        "id": "Acr-XVHpKa36"
      },
      "id": "Acr-XVHpKa36",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPDomdAf1dv",
        "outputId": "8fb214a1-3a8d-44f0-f05d-f40c22bb737f"
      },
      "id": "eNPDomdAf1dv",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'现身说法'},\n",
              " {'赞不绝口', '鬼斧神工'},\n",
              " {'依依不舍'},\n",
              " {'先天不足'},\n",
              " {'瞠目结舌'},\n",
              " {'十拿九稳'},\n",
              " {'堂而皇之'},\n",
              " {'昂首阔步'},\n",
              " {'山穷水尽'},\n",
              " {'照本宣科'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('mT5-small trained on 20000 data: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "id": "G8H7iPD_KcWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb3c1ac-3044-44ec-914b-1a3fa2783f2e"
      },
      "id": "G8H7iPD_KcWW",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mT5-small trained on 20000 data: \n",
            "Accuracy for test set is 0.4234357848518112\n",
            "F1 score for test set is 0.42419243986254296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40389dde-7b44-44b7-b9d3-e3550cedb021",
      "metadata": {
        "id": "40389dde-7b44-44b7-b9d3-e3550cedb021"
      },
      "source": [
        "## Can the Fine-tuned Model Understand Similar Instruction(s)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "363fcf61-ccca-4d4f-832d-04a85e4b2b44",
      "metadata": {
        "id": "363fcf61-ccca-4d4f-832d-04a85e4b2b44"
      },
      "outputs": [],
      "source": [
        "def prompt_2(data):\n",
        "    '''data shall be the output of `read_data`'''\n",
        "    text_input = []\n",
        "    \n",
        "    for i in range(len(data)):\n",
        "        # data[i] = eval(data[i])\n",
        "        input_text = data[i]['content']\n",
        "        candidates = data[i]['candidates']\n",
        "\n",
        "        candidate_str = ''\n",
        "        for candidate in candidates:\n",
        "            candidate_str += '('+'|'.join(candidate)+')'\n",
        "        \n",
        "        preprocess_idx = -1\n",
        "        def replace(match):\n",
        "            nonlocal preprocess_idx\n",
        "            preprocess_idx += 1\n",
        "            return 'extra{}'.format(preprocess_idx)\n",
        "        input_text = re.sub(r'#idiom#', replace, input_text)\n",
        "\n",
        "        instruction = '请依次选择括号里的成语填空：{}'.format(candidate_str)\n",
        "        \n",
        "        text_input.append(instruction+'\\n'+input_text)\n",
        "\n",
        "    return text_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_2 = prompt_2(test_set)\n",
        "print(test_input_2[0])\n",
        "answer_fn(tuned_t5_cn, test_input_2[0], bert_tokenizer)"
      ],
      "metadata": {
        "id": "jro5uGWeOS2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "fd13abc4-ef27-493c-8f3d-609dc92c50b7"
      },
      "id": "jro5uGWeOS2q",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请依次选择括号里的成语填空：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'白日做梦开点和,远所定尽,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_2 = prompt_2(test_set)\n",
        "print(test_input_2[0])\n",
        "answer_fn(tuned_mt5, test_input_2[0], auto_tokenizer)"
      ],
      "metadata": {
        "id": "ehYSBfJ3Odir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6c7c5fc7-28d4-4483-a6b3-be9ba5de6bce"
      },
      "id": "ehYSBfJ3Odir",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "请依次选择括号里的成语填空：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
            "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'现身说法,白日做梦+uzo真ensya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems not... Let's see some more data!"
      ],
      "metadata": {
        "id": "73yy6pNb9z0_"
      },
      "id": "73yy6pNb9z0_"
    },
    {
      "cell_type": "code",
      "source": [
        "sys = []\n",
        "for q in test_input_2[:100]:  \n",
        "    a = answer_fn(tuned_t5_cn, q, bert_tokenizer)\n",
        "    a = a.strip(' .。，#<>').split(',')\n",
        "    sys.append(set(a))"
      ],
      "metadata": {
        "id": "9CNQNFJV8pgl"
      },
      "id": "9CNQNFJV8pgl",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1, tp = f1_score(sys, gold_text)\n",
        "print('Use different prompts as what was trained on T5-small after fine-tuning: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold_text, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkXsFYDC-CGs",
        "outputId": "661ca8c8-6195-4bea-e3dd-02e16c515c70"
      },
      "id": "CkXsFYDC-CGs",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use different prompts as what was trained on T5-small after fine-tuning: \n",
            "Accuracy for test set is 0.05982905982905983\n",
            "F1 score for test set is 0.04142011834319526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MuUHU4G-ECD",
        "outputId": "a2ffeae4-923e-4900-b2ef-66371f05e157"
      },
      "id": "5MuUHU4G-ECD",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'分依', '动', '白日做梦'},\n",
              " {'', '画龙点睛', '立同价不', '赞不绝口'},\n",
              " {'依居不舍之', '无一风有', '无不', '风'},\n",
              " {'之可迷动心', '先天不足有'},\n",
              " {'心地一人', '骇人听闻动'},\n",
              " {'凶多吉少入水高长一'},\n",
              " {'', '不心手', '招摇过市可神'},\n",
              " {'名言人可行人', '昂首阔步心分', '知地'},\n",
              " {'孤注一掷地心地'},\n",
              " {'', '光照本宣科定一', '大'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys = []\n",
        "for q in test_input_2[:100]:  \n",
        "    a = answer_fn(tuned_mt5, q, auto_tokenizer)\n",
        "    a = a.replace('_id_', '').strip(' .。，#<>').split(',')\n",
        "    sys.append(set(a))"
      ],
      "metadata": {
        "id": "Dy0nZzAM9g8u"
      },
      "id": "Dy0nZzAM9g8u",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, f1, tp = f1_score(sys, gold_text)\n",
        "print('Use different prompts as what was trained on mT5-small after fine-tuning: ')\n",
        "print(f\"Accuracy for test set is {accuracy(sys, gold_text, tp)}\")\n",
        "print(f\"F1 score for test set is {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqRomw5i-Knq",
        "outputId": "b6d1975c-8fd2-476e-8747-bf36e7be1b19"
      },
      "id": "xqRomw5i-Knq",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use different prompts as what was trained on mT5-small after fine-tuning: \n",
            "Accuracy for test set is 0.1623931623931624\n",
            "F1 score for test set is 0.08539325842696631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJWxhRbVAAxX",
        "outputId": "6d867e30-5cad-4225-f631-b0c61406e79f"
      },
      "id": "RJWxhRbVAAxX",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'现身说法', '白日做梦<xD>lerken了下来说一点后来迟疑了一些真相走了出来以下是一本书铺的信息填空并如单了起来的过程'},\n",
              " {'拍案叫绝',\n",
              "  '诚心悦信后当五感慨ensyawność勉вачкаReturn至衷地说lerken以下是口腔音词填空了!”泣为观止dunk力forward义标引来深いただきたい谈以此推手hemiksipanFinishவையும்称惜望度缓keet迟提abilitymezichtelimkupusius谅itingValue精神полните',\n",
              "  '赞不尽服enschaft敬谢意念惭Feedback扬眉吐气шимиCancel赏道声Receive明华说纷纷!)'},\n",
              " {'>bes',\n",
              "  '依循无间',\n",
              "  '相随两人之间在一起的合作以此左右untersch地舍之下的结果填空为期据此交际<xD><x>lerken置虚することに息念naha说并行<'},\n",
              " {'>一言为定Baolaureat',\n",
              "  '归根结蒂',\n",
              "  '权宜之计<xF>过度komandialiteit<xD>ensya+<x><x><xF>优良价值<'},\n",
              " {'>+menti不安zmagojatklini说Tende空而来+<',\n",
              "  '><x><xD><xFD>bes<xA>相守<x><',\n",
              "  '>hedenAktualSerbi息',\n",
              "  '>ienti悔<x>可想而知<xF><xD>Студ域名填补并论<x><x><',\n",
              "  '时不我待暴露真相+骇人听闻<',\n",
              "  '瞠目结舌'},\n",
              " {'><x><xC><xE>暴露出来portfoliotendent空升息<',\n",
              "  '><xD>+diantuv外<x>据说势趋之下俱低lerken并论较弱komandi<',\n",
              "  '>vista为顶上线<x><x><',\n",
              "  '>倒去Vokaliteit稍逊别<xF',\n",
              "  '>拉客战下盘<xF><xFD><',\n",
              "  '十拿九稳',\n",
              "  '高开主胜若明+<x>aru<'},\n",
              " {'>+',\n",
              "  '>meni谅落下了水线经过别论<',\n",
              "  '>看上去老实一样<',\n",
              "  '>顶上天立地<xF><x><',\n",
              "  '堂而皇之',\n",
              "  '指望下帝了一点光光明地走了如此一丈niego<x><x>他<xF>掉下去说出来了他不过神来谈的感觉тую并靠近前道而来besensya<x>lerken以下训练有素<xFD>填空的信息图绘就下来发生了后合<xF>+<x>mitteionnaire当局来说后来过了那种东西好好的暴露真相untersch原本无穷意味地说不定据为我/>正好你活过气<'},\n",
              " {'昂首阔步',\n",
              "  '精神抖擞ensyalerken填空并如说挈keidenKuli作样<x>odno无言之语标拉待Nahameni提落komandi训练表ējām'},\n",
              " {'>odnonise原本发挥<x><',\n",
              "  '>uvanbesedildivista测绘词tamamla得相当艰难mezi之间提高了进攻setTimeout合语缚来的态度döndü人<x>kininmapumittemerautovçalıştı用稍自行+<xF>zionali提名谈Sumazette成效之一uedbet赫塔不动<',\n",
              "  '>zmag并以此为回计Ergebnisdüzenlelerken了敌手训练的效果发生了后来调下komand权势以东一面据说他试图从此有效走了迟缓<xF>+合作+<xF><',\n",
              "  '>深入\\u1ccbdajiærdighed的机会',\n",
              "  '一败涂地暴露掉后卫金相植填空<xD>aru<',\n",
              "  '营私舞弊'},\n",
              " {'>+汉',\n",
              "  '>asiya+以此为主说<xD>hutu',\n",
              "  '照本宣科<x>ensya填空并移下去<xD><x>做实成规<x>aru<xF>çat有机<x>çatdıraymizkomandiolsun<x><x><',\n",
              "  '闭门造车<xE><'}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks that the fine-tuned models can make some predictions, and they work better than those before fine-tuning. However, it shows that the proper prompts highly rely on the training data during fine-tune, even after fine-tuning, which indicates that the fine-tuned model cannot really understand the instructions. \n",
        "\n",
        "Given this primary conclusion, if we have extra time, we may simplify the data preprocessing."
      ],
      "metadata": {
        "id": "fIl97IPOGWei"
      },
      "id": "fIl97IPOGWei"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeU_h9idACMK"
      },
      "id": "KeU_h9idACMK",
      "execution_count": 42,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}