{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20bb81a-4603-480a-9c59-449a594dfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e374754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3d4aa2-66e8-46b6-8cad-9c88da2844f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "\n",
    "    for example in data:\n",
    "        example = json.loads(example)\n",
    "\n",
    "        input_text = example['content']\n",
    "        ground_truth = example['groundTruth']\n",
    "        candidates = example['candidates']\n",
    "        \n",
    "        for i, idiom in enumerate(ground_truth):\n",
    "            candidates_str = '，'.join([c for c in candidates[i]])\n",
    "            input_text = input_text.replace('#idiom#', f\"#[MASK][MASK][MASK][MASK]#({candidates})\", 1)\n",
    "            # input_text = input_text.replace('#idiom#', f\"#[MASK][MASK][MASK][MASK]#({candidates_str})\", 1)\n",
    "        if len(input_text) > 500:\n",
    "            continue  \n",
    "        input_texts.append(input_text)\n",
    "        labels.append(''.join(ground_truth))\n",
    "\n",
    "\n",
    "    concat_inputs = tokenizer(input_texts,return_token_type_ids=False)\n",
    "    labels = tokenizer(labels, return_token_type_ids=False, return_attention_mask=False, add_special_tokens=False)\n",
    "    return concat_inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6735dcd-f623-418d-bb3a-bdf1183c224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdiomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels) -> None:\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.labels = self._get_label(inputs[\"input_ids\"], labels[\"input_ids\"])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "    \n",
    "    def _get_label(self, inputs, labels):\n",
    "        results = []\n",
    "        for inp, label in zip(inputs, labels):\n",
    "            inp = np.array(inp)\n",
    "            l = np.full_like(inp, fill_value=-100)\n",
    "            l[inp == tokenizer.mask_token_id] = label\n",
    "            results.append(l)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"input_ids\": self.inputs[\"input_ids\"][index], \"attention_mask\": self.inputs[\"attention_mask\"][index], \"labels\": self.labels[index]}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input_ids = [torch.LongTensor(each[\"input_ids\"]) for each in batch]\n",
    "    batch_att_mask = [torch.LongTensor(each[\"attention_mask\"]) for each in batch]\n",
    "    batch_label = [torch.LongTensor(each[\"labels\"]) for each in batch]\n",
    "    padded_batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = pad_sequence(batch_att_mask, batch_first=True, padding_value=0)\n",
    "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
    "    \n",
    "def to_device(data, device):\n",
    "    new_data = {}\n",
    "    for k in data:\n",
    "        new_data[k] = data[k].to(device)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17f646e-72cb-42a9-930e-c5f9bc1cdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train_loader:DataLoader, optimizer:optim.Optimizer, log_step=100):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    log_loss = 0.0\n",
    "    for idx, batch in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = to_device(batch, device)\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        log_loss += loss.item()\n",
    "        if idx % log_step == 0:\n",
    "            print(f\"Train Step: {idx} Loss: {log_loss / log_step}\")\n",
    "            log_loss = 0.0\n",
    "    return epoch_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for batch in eval_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        eval_loss += loss.item()\n",
    "        pred = output.logits.argmax(-1)\n",
    "        label = batch[\"labels\"]\n",
    "        correct += torch.where(label!=-100, pred==label, 0).sum().item()\n",
    "        total += torch.sum(label != -100).item()\n",
    "\n",
    "    eval_acc = correct / total\n",
    "    eval_loss = eval_loss / len(eval_loader) \n",
    "    print(total, correct)\n",
    "    return eval_acc, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835f25f7-7e8d-438b-a2eb-f838103a66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chinese Idioms dataset\n",
    "train_data_file = './data/train_15000.txt'\n",
    "val_data_file = './data/dev_2000.txt'\n",
    "\n",
    "\n",
    "with open(train_data_file) as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "with open(val_data_file) as f:\n",
    "    val_data = f.readlines()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\")\n",
    "\n",
    "train_inputs, train_labels = preprocess_data(train_data)\n",
    "val_inputs, val_labels = preprocess_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ac227c-6c24-4ab8-9af4-572a9808c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 深 恶 痛 绝 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 3918 2626 4578 5318 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100]\n",
      "[CLS] 另 据 了 解 ， 北 京 一 个 对 垃 圾 短 信 # [MASK] [MASK] [MASK] [MASK] # ( [ ['深 恶 痛 绝 ','人 人 自 危 ','恨 入 骨 髓 ','不 胜 枚 举 ','嗤 之 以 鼻 ','走 马 看 花 ','不 屑 一 顾'] ] ) 的 老 人 ， 利 用 该 软 件 总 共 呼 死 了 近 2000 个 号 码 。 20 分 钟 呼 上 万 号 码 记 者 昨 天 在 百 度 里 输 入 [UNK] 呼 死 你 软 件 [UNK] ， 出 现 了 7000 多 个 相 关 网 页 ， 随 机 登 录 几 个 网 站 ， 发 现 软 件 均 需 花 钱 购 买 ， 价 格 从 200 元 至 500 元 不 等 。 [SEP]\n",
      "tensor([[ 101, 1369, 2945,  ...,    0,    0,    0],\n",
      "        [ 101, 6821,  671,  ...,    0,    0,    0],\n",
      "        [ 101,  124, 3299,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3187, 3144,  ...,    0,    0,    0],\n",
      "        [ 101, 4197, 1400,  ...,    0,    0,    0],\n",
      "        [ 101, 2190,  754,  ...,    0,    0,    0]])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "val_dataset = IdiomDataset(val_inputs, val_labels)\n",
    "print(tokenizer.decode(val_dataset[0][\"labels\"]))\n",
    "print(val_dataset[0][\"labels\"])\n",
    "print(tokenizer.decode(val_dataset[0][\"input_ids\"]))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "for each in val_loader:\n",
    "    print(each[\"input_ids\"])\n",
    "    print(each.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d709d73-eb41-4156-a496-44580e614721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IdiomDataset(train_inputs, train_labels)\n",
    "val_dataset = IdiomDataset(val_inputs, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b7aae8-b731-4282-82a6-c7249fa7a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n",
      "Train Step: 100 Loss: 2.9327831733226777\n",
      "Train Step: 200 Loss: 2.3315607142448425\n",
      "Train Step: 300 Loss: 2.2123726773262025\n",
      "Train Step: 400 Loss: 2.1301386761665344\n",
      "Train Step: 500 Loss: 2.0706395006179807\n",
      "Train Step: 600 Loss: 1.9072985714673996\n",
      "Train Step: 700 Loss: 1.8154919040203095\n",
      "Train Step: 800 Loss: 1.7903302580118179\n",
      "Epoch 1 Training Loss: 2.1094888179923124\n",
      "8452 5045\n",
      "Epoch 1 Eval Acc: 0.59690014197823; Eval Loss: 1.383521120337879\n",
      "Training Epoch 2\n",
      "Train Step: 100 Loss: 1.1090427428483962\n",
      "Train Step: 200 Loss: 1.132837101817131\n",
      "Train Step: 300 Loss: 1.0888219580054284\n",
      "Train Step: 400 Loss: 1.1478842985630036\n",
      "Train Step: 500 Loss: 1.1079305881261825\n",
      "Train Step: 600 Loss: 1.1348290991783143\n",
      "Train Step: 700 Loss: 1.1474296295642852\n",
      "Train Step: 800 Loss: 1.1295168882608413\n",
      "Epoch 2 Training Loss: 1.1243057316494633\n",
      "8452 5528\n",
      "Epoch 2 Eval Acc: 0.6540463795551349; Eval Loss: 1.2507230892401784\n",
      "Training Epoch 3\n",
      "Train Step: 100 Loss: 0.5392642591893673\n",
      "Train Step: 200 Loss: 0.5807422452419996\n",
      "Train Step: 300 Loss: 0.6278130192309618\n",
      "Train Step: 400 Loss: 0.6306415888667106\n",
      "Train Step: 500 Loss: 0.6449482934176922\n",
      "Train Step: 600 Loss: 0.6473021684587001\n",
      "Train Step: 700 Loss: 0.5864362348616123\n",
      "Train Step: 800 Loss: 0.6385798817873001\n",
      "Epoch 3 Training Loss: 0.6167335328147269\n",
      "8452 5638\n",
      "Epoch 3 Eval Acc: 0.6670610506389021; Eval Loss: 1.4872430542937847\n",
      "Training Epoch 4\n",
      "Train Step: 100 Loss: 0.33971525978296996\n",
      "Train Step: 200 Loss: 0.3832809391617775\n",
      "Train Step: 300 Loss: 0.368456614613533\n",
      "Train Step: 400 Loss: 0.4109527396410704\n",
      "Train Step: 500 Loss: 0.3967733270302415\n",
      "Train Step: 600 Loss: 0.405269905179739\n",
      "Train Step: 700 Loss: 0.4160869414359331\n",
      "Train Step: 800 Loss: 0.4134859775751829\n",
      "Epoch 4 Training Loss: 0.39372746836238676\n",
      "8452 5736\n",
      "Epoch 4 Eval Acc: 0.6786559394226218; Eval Loss: 1.602996808641097\n",
      "Training Epoch 5\n",
      "Train Step: 100 Loss: 0.2339111070893705\n",
      "Train Step: 200 Loss: 0.23047998027876018\n",
      "Train Step: 300 Loss: 0.2570111934654415\n",
      "Train Step: 400 Loss: 0.26730963088572024\n",
      "Train Step: 500 Loss: 0.24417349831201135\n",
      "Train Step: 600 Loss: 0.26774229042232034\n",
      "Train Step: 700 Loss: 0.27856961272656916\n",
      "Train Step: 800 Loss: 0.2814443671889603\n",
      "Epoch 5 Training Loss: 0.2614149164960729\n",
      "8452 5722\n",
      "Epoch 5 Eval Acc: 0.6769995267392334; Eval Loss: 1.811782294461707\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\").to(device)\n",
    "\n",
    "epoches = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, epoches+1):\n",
    "    print(f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch} Training Loss: {train_loss}\")\n",
    "    \n",
    "    eval_acc, eval_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n",
    "torch.save(model.state_dict(), \"bert_ckpt_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f96111-1454-4ffd-9f3d-de953278bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_idiom(model, loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for batch in loader:   \n",
    "        batch = to_device(batch, device)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = output.logits.argmax(-1)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            idiom_idx = label != -100\n",
    "            l = label[idiom_idx].split(4)\n",
    "            p = pred[idiom_idx].split(4)\n",
    "            gold_idiom =  set(\"\".join(tokenizer.convert_ids_to_tokens(i)) for i in l)\n",
    "            pred_idiom = set(\"\".join(tokenizer.convert_ids_to_tokens(i)) for i in p)\n",
    "            all_labels.append(set(gold_idiom))\n",
    "            all_preds.append(set(pred_idiom))\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "        \n",
    "\n",
    "def f1_score(sys, gold):\n",
    "    tp = 0\n",
    "    t = 0\n",
    "    p = 0\n",
    "    for s, g in zip(sys, gold):\n",
    "        t += len(g)\n",
    "        p += len(s)\n",
    "        tp += len(g & s)\n",
    "    precision = tp / p if p != 0 else 0\n",
    "    recall = tp / t if t != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return precision, recall, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e254f55-b16a-4ef8-8c65-09bac7978272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\", cache_dir=\"./models\").to(device)\n",
    "model.load_state_dict(torch.load(\"bert_ckpt_new.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b934ffd-6b88-4580-8137-23690a0719ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5450664136622391 0.5448079658605974 0.5449371591178563\n"
     ]
    }
   ],
   "source": [
    "sys, gold = fill_idiom(model, val_loader)\n",
    "p, r, f1 = f1_score(sys, gold)\n",
    "print(p, r, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
