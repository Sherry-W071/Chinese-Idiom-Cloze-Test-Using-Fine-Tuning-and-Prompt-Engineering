{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "133ca7f8-a02c-4605-bf97-ba80b5547c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import cuda, nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import GPT2LMHeadModel, BertTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ef6474-6b2a-4da9-98ad-f3b67caed9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46376977f83243aeb22e01ce5d78e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519e555268a74110b76b39015e060e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c297ba4beb54394a8a29dd7d6668e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc036d6c545445ce8174e6a4fe9c634e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f0b6fe100c4835a05bc23ff76e71c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\", cache_dir=\"./models\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\", cache_dir=\"./models\")\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de05c276-51c7-4b4c-a1df-3fdf10b9acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the inputs and outputs\n",
    "def preprocess_data(data):\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "\n",
    "    for example in data:\n",
    "        example = json.loads(example)\n",
    "\n",
    "        input_text = example['content']\n",
    "        ground_truth = example['groundTruth']\n",
    "        candidates = example['candidates']\n",
    "        \n",
    "        for i, idiom in enumerate(ground_truth):\n",
    "            input_text = \"请从（）里选择出最合适的成语: \" + input_text\n",
    "            candidates_str = '|'.join([c for c in candidates[i]])\n",
    "            input_text = input_text.replace('#idiom#', \"（\" + candidates_str + \"）\", 1)\n",
    "        if len(input_text) > 500:\n",
    "            continue  \n",
    "        input_texts.append(input_text)\n",
    "        labels.append('、'.join(ground_truth))\n",
    "\n",
    "\n",
    "    concat_inputs = tokenizer(input_texts, labels, return_token_type_ids=False)\n",
    "    return concat_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "003a044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdiomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = self._get_label(data[\"input_ids\"])\n",
    "        self.is_inference = False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "    \n",
    "    def _get_label(self, inputs):\n",
    "        labels = []\n",
    "        for inp in inputs:\n",
    "            sep_idx = inp.index(102)\n",
    "            label = [-100] * len(inp)\n",
    "            label[sep_idx + 1:] = inp[sep_idx + 1:]\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "\n",
    "    def inference(self):\n",
    "        self.is_inference = True\n",
    "\n",
    "    def train(self):\n",
    "        self.is_inference = False\n",
    "        \n",
    "    def is_inference(self):\n",
    "        return self.is_inference\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not self.is_inference:\n",
    "            return {\"input_ids\": self.data[\"input_ids\"][index], \"attention_mask\": self.data[\"attention_mask\"][index], \"labels\": self.label[index]}\n",
    "        else:\n",
    "            sep_idx = self.data[\"input_ids\"][index].index(102)\n",
    "            input_ids = self.data[\"input_ids\"][index][:sep_idx+1]\n",
    "            att_mask = self.data[\"attention_mask\"][index][:sep_idx+1]\n",
    "            label = self.label[index][sep_idx+1:]\n",
    "            return {\"input_ids\": input_ids, \"attention_mask\":att_mask, \"labels\":label}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input_ids = [torch.LongTensor(each[\"input_ids\"]) for each in batch]\n",
    "    batch_att_mask = [torch.LongTensor(each[\"attention_mask\"]) for each in batch]\n",
    "    batch_label = [torch.LongTensor(each[\"labels\"]) for each in batch]\n",
    "    padded_batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = pad_sequence(batch_att_mask, batch_first=True, padding_value=0)\n",
    "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
    "\n",
    "def to_device(data, device):\n",
    "    new_data = {}\n",
    "    for k in data:\n",
    "        new_data[k] = data[k].to(device)\n",
    "    return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6243bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train_loader:DataLoader, optimizer:optim.Optimizer, log_step=100):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    log_loss = 0.0\n",
    "    for idx, batch in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = to_device(batch, device)\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        log_loss += loss.item()\n",
    "        if idx % log_step == 0:\n",
    "            print(f\"Train Step: {idx} Loss: {log_loss / log_step}\")\n",
    "            log_loss = 0.0\n",
    "    return epoch_loss / len(train_loader)\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for batch in eval_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        eval_loss += loss.item()\n",
    "        pred = output.logits.argmax(-1)[..., :-1]\n",
    "        label = batch[\"labels\"][..., 1:]\n",
    "        correct += torch.where(label!=-100, pred==label, 0).sum().item()\n",
    "        total += torch.sum(label != -100).item()\n",
    "\n",
    "    eval_acc = correct / total\n",
    "    eval_loss = eval_loss / len(eval_loader) \n",
    "    print(total, correct)\n",
    "    return eval_acc, eval_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aeeac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chinese Idioms dataset\n",
    "train_data_file = './data/train_15000.txt'\n",
    "val_data_file = './data/dev_2000.txt'\n",
    "\n",
    "\n",
    "with open(train_data_file) as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "with open(val_data_file) as f:\n",
    "    val_data = f.readlines()\n",
    "\n",
    "train_inputs = preprocess_data(train_data)\n",
    "val_inputs = preprocess_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46fd451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IdiomDataset(train_inputs)\n",
    "val_dataset = IdiomDataset(val_inputs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa3424-bcaa-4b10-a081-99a92db27b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n",
      "Train Step: 100 Loss: 0.9795388734340668\n",
      "Train Step: 200 Loss: 0.7113763672113419\n",
      "Train Step: 300 Loss: 0.6371003955602645\n",
      "Train Step: 400 Loss: 0.5993250006437302\n",
      "Train Step: 500 Loss: 0.5624085527658462\n",
      "Train Step: 600 Loss: 0.5670115682482719\n",
      "Train Step: 700 Loss: 0.5523709109425545\n",
      "Train Step: 800 Loss: 0.5443778941035271\n",
      "Train Step: 900 Loss: 0.5170685809850692\n",
      "Epoch 1 Training Loss: 0.6248667251898535\n",
      "11950 10259\n",
      "Epoch 1 Eval Acc: 0.8584937238493724; Eval Loss: 0.45387884497642517\n",
      "Training Epoch 2\n",
      "Train Step: 100 Loss: 0.420941047668457\n",
      "Train Step: 200 Loss: 0.409544515311718\n",
      "Train Step: 300 Loss: 0.4202533406019211\n",
      "Train Step: 400 Loss: 0.4160501465201378\n",
      "Train Step: 500 Loss: 0.4195918321609497\n",
      "Train Step: 600 Loss: 0.40703207135200503\n",
      "Train Step: 700 Loss: 0.4191902096569538\n",
      "Train Step: 800 Loss: 0.4008663776516914\n",
      "Train Step: 900 Loss: 0.3856555975973606\n",
      "Epoch 2 Training Loss: 0.41043707115182493\n",
      "11950 10432\n",
      "Epoch 2 Eval Acc: 0.8729707112970712; Eval Loss: 0.4036372262239456\n",
      "Training Epoch 3\n",
      "Train Step: 100 Loss: 0.2973900564014912\n",
      "Train Step: 200 Loss: 0.3024574266374111\n",
      "Train Step: 300 Loss: 0.31151023879647255\n",
      "Train Step: 400 Loss: 0.29613494262099266\n",
      "Train Step: 500 Loss: 0.29277900062501433\n",
      "Train Step: 600 Loss: 0.2978680573403835\n",
      "Train Step: 700 Loss: 0.29680510975420477\n",
      "Train Step: 800 Loss: 0.2911746095120907\n",
      "Train Step: 900 Loss: 0.2959188075363636\n",
      "Epoch 3 Training Loss: 0.2975854568008675\n",
      "11950 10564\n",
      "Epoch 3 Eval Acc: 0.8840167364016737; Eval Loss: 0.37734932255744935\n",
      "Training Epoch 4\n",
      "Train Step: 100 Loss: 0.20362456314265728\n",
      "Train Step: 200 Loss: 0.19051185987889765\n",
      "Train Step: 300 Loss: 0.18542552404105664\n",
      "Train Step: 400 Loss: 0.1886687069386244\n",
      "Train Step: 500 Loss: 0.2192094425112009\n",
      "Train Step: 600 Loss: 0.19757893316447736\n",
      "Train Step: 700 Loss: 0.19795122899115086\n",
      "Train Step: 800 Loss: 0.20947745248675345\n",
      "Train Step: 900 Loss: 0.20506541833281516\n",
      "Epoch 4 Training Loss: 0.19942590589525858\n",
      "11950 10613\n",
      "Epoch 4 Eval Acc: 0.8881171548117155; Eval Loss: 0.3873129894733429\n",
      "Training Epoch 5\n",
      "Train Step: 100 Loss: 0.12669909976422786\n",
      "Train Step: 200 Loss: 0.12408816132694483\n"
     ]
    }
   ],
   "source": [
    "epoches = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, epoches+1):\n",
    "    print(f\"Training Epoch {epoch}\")\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch} Training Loss: {train_loss}\")\n",
    "    eval_acc, eval_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n",
    "torch.save(model.state_dict(), \"gpt2_ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a79838be-531d-4ec0-8598-035da5ffcfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if all((input_ids==self.keywords[0]).sum(dim=-1) >= 2):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "stop_words = [\"[SEP]\"]\n",
    "stop_ids = [tokenizer.convert_tokens_to_ids(w) for w in stop_words]\n",
    "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
    "stop_criteria_list = StoppingCriteriaList([stop_criteria])\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_idiom(model, loader):\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for batch in loader:   \n",
    "        batch = to_device(batch, device)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        # pos_ids = batch[\"position_ids\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, return_dict_in_generate=True, pad_token_id=50256, max_length=512, top_k=10, stopping_criteria=stop_criteria_list)\n",
    "        pred_start = torch.nonzero(input_ids==tokenizer.sep_token_id, as_tuple=True)[1][0] + 1\n",
    "        truncated_outputs = []\n",
    "        for out in outputs[\"sequences\"]:\n",
    "            sep_idxs = torch.nonzero(out==tokenizer.sep_token_id, as_tuple=True)[0]\n",
    "            if len(sep_idxs) == 1:\n",
    "                end_idx = -1\n",
    "            else:\n",
    "                end_idx = sep_idxs[1]\n",
    "            truncated_outputs.append(out[pred_start:end_idx])\n",
    "        decode_texts = tokenizer.batch_decode(truncated_outputs)\n",
    "        gold_texts = tokenizer.batch_decode([l[l != -100][:-1] for l in labels])\n",
    "\n",
    "        for gold, decode in zip(gold_texts, decode_texts):\n",
    "            l = set(gold.replace(\" \", \"\").split(\"、\"))\n",
    "            p = set(decode.replace(\" \", \"\").split(\"、\"))\n",
    "            all_labels.append(l)\n",
    "            all_preds.append(p)\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "        \n",
    "\n",
    "def f1_score(sys, gold):\n",
    "    tp = 0\n",
    "    t = 0\n",
    "    p = 0\n",
    "    for s, g in zip(sys, gold):\n",
    "        t += len(g)\n",
    "        p += len(s)\n",
    "        tp += len(g & s)\n",
    "    precision = tp / p if p != 0 else 0\n",
    "    recall = tp / t if t != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def left_pad_sequence(sequence, batch_first, padding_value=0):\n",
    "    padded = []\n",
    "    max_len = max(len(each) for each in sequence)\n",
    "    for each in sequence:\n",
    "        if not isinstance(each, torch.LongTensor):\n",
    "            each = torch.LongTensor(each)\n",
    "        pad = torch.full((max_len-len(each),), fill_value=padding_value,dtype=each.dtype)\n",
    "        padded.append(torch.cat([pad, each]))\n",
    "    padded = torch.vstack(padded)\n",
    "    if not batch_first:\n",
    "        padded = padded.permute(1, 0, 2)\n",
    "    return padded\n",
    "        \n",
    "def inference_colate_fn(batch):\n",
    "    batch_input_ids = [torch.LongTensor(each[\"input_ids\"]) for each in batch]\n",
    "    batch_att_mask = [torch.LongTensor(each[\"attention_mask\"]) for each in batch]\n",
    "    batch_label = [torch.LongTensor(each[\"labels\"]) for each in batch]\n",
    "    batch_position_ids = [torch.arange(len(each[\"input_ids\"]), dtype=torch.long) for each in batch]\n",
    "    \n",
    "    padded_batch_input_ids = left_pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = left_pad_sequence(batch_att_mask, batch_first=True, padding_value=0)\n",
    "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=-100)\n",
    "    # padded_batch_position_ids = left_pad_sequence(batch_position_ids, batch_first=True, padding_value=0)\n",
    "    # return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"position_ids\":padded_batch_position_ids, \"labels\": padded_batch_label}   \n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}    \n",
    "\n",
    "val_dataset.inference()\n",
    "inf_loader = DataLoader(val_dataset, batch_size=64, collate_fn=inference_colate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5ef41417-1090-4bba-a9b8-8b359a7c06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46860514117151286 0.4672268907563025 0.46791500105196715\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2_ckpt.pt\", map_location=device))\n",
    "sys, gold = fill_idiom(model, loader=inf_loader)\n",
    "p, r, f1 = f1_score(sys, gold)\n",
    "print(p, r, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a6045c28-5201-45de-8c64-c8ec8e39a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in inf_loader:   \n",
    "#     batch = to_device(batch, device)\n",
    "#     input_ids = batch[\"input_ids\"]\n",
    "#     attention_mask = batch[\"attention_mask\"]\n",
    "#     pos_ids = batch[\"position_ids\"]\n",
    "#     labels = batch[\"labels\"]\n",
    "#     outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, position_ids=pos_ids, return_dict_in_generate=True, pad_token_id=50256, max_length=512, top_k=10, stopping_criteria=stop_criteria_list)\n",
    "#     res = tokenizer.batch_decode(outputs[\"sequences\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c360d9c9-930a-48bb-9120-4b84bd8fe2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in inf_loader:   \n",
    "#     batch = to_device(batch, device)\n",
    "#     input_ids = batch[\"input_ids\"]\n",
    "#     attention_mask = batch[\"attention_mask\"]\n",
    "#     labels = batch[\"labels\"]\n",
    "#     outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, return_dict_in_generate=True, pad_token_id=50256, max_length=512, top_k=10, stopping_criteria=stop_criteria_list)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
