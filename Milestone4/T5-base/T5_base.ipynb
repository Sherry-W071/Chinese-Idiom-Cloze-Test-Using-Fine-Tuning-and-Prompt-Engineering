{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFuLmsTRP3j_"
      },
      "source": [
        "# Milestone 3 - T5_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ek2gPUkUcvJ"
      },
      "outputs": [],
      "source": [
        "dataset = [\"\"\"{\n",
        "    \"content\": \"世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得”，就需要各方面密切配合、#idiom#。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、#idiom#、灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。\",\n",
        "    \"realCount\": 2,\n",
        "    \"groundTruth\": [\"通力合作\", \"有的放矢\"],\n",
        "    \"candidates\": [\n",
        "        [\"凭空捏造\", \"高头大马\", \"通力合作\", \"同舟共济\", \"和衷共济\", \"蓬头垢面\", \"紧锣密鼓\"],\n",
        "        [\"叫苦连天\", \"量体裁衣\", \"金榜题名\", \"百战不殆\", \"知彼知己\", \"有的放矢\", \"风流才子\"]\n",
        "    ]\n",
        "}\"\"\"]\n",
        "s = \"世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得”，就需要各方面密切配合、通力合作。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、</s>、灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。可选成语有：叫苦连天 | 量体裁衣 | 金榜题名 | 百战不殆 | 知彼知己 | 风流才子，请选出最佳的一项。\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suiV4bulTUQM",
        "outputId": "802765ca-5a9c-46a0-f7d5-ffbc51d750e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.98\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnPrGWihP53M",
        "outputId": "fa6ff30b-0982-4c71-f413-197503c42379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch import cuda, nn, optim\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "path = '/content/drive/MyDrive/ChID-Dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D9H_21qtAs1"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsESQ0nHQ0e0",
        "outputId": "e85904e8-6bed-4de6-e630-60a8d272c061"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\") \n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8j7sQPrtAs1"
      },
      "source": [
        "# Fine-tuning model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ipS4_og1Yi",
        "tags": []
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMvTMB9TtAs1"
      },
      "outputs": [],
      "source": [
        "def prompt1(text, candidates, choice):\n",
        "    input_text = \"请从（）里选择出最合适的成语: \" + text\n",
        "    for i in range(len(choice)):\n",
        "        candidates_str = '|'.join([c for c in candidates[i]])\n",
        "        input_text = input_text.replace('#idiom#', \"（\" + candidates_str + \"）\", 1) \n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSlnaXlgtAs2"
      },
      "outputs": [],
      "source": [
        "def prompt2(text, candidates, choice):\n",
        "    input_text = text.replace('#idiom#', \"_\")\n",
        "    input_text = \"请依次从（）选择出最合适的成语填入_: \" + input_text\n",
        "    for i in range(len(choice)):\n",
        "        candidates_str = '|'.join([c for c in candidates[i]])\n",
        "        input_text = input_text.replace('（）', \"（\" + candidates_str + \"）（）\", 1)\n",
        "    input_text = input_text.replace(\"（）\", \"\")\n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWlPSnLotAs2"
      },
      "outputs": [],
      "source": [
        "def prompt3(text, candidates, choice):\n",
        "    ex = '''选择：[[“凭空捏造“, “高头大马“, “通力合作“, “同舟共济“, “和衷共济“, “蓬头垢面“, “紧锣密鼓“],[“叫苦连天“, “量体裁衣“, “金榜题名“, “百战不殆“, “知彼知己“, “有的放矢“, “风流才子“]]\\n输入：“世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得“，就需要各方面密切配合、#idiom#。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、#idiom#、灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。”\\n输出：通力合作, 有的放矢\\n'''\n",
        "    input_text = ex\n",
        "    input_text = input_text + f\"选择：{candidates}\\n输入：{text}\\n输出：\"\n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYXRTot2tAs2"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, prompt):\n",
        "    input_texts = []\n",
        "    labels = []\n",
        "\n",
        "    for example in data:\n",
        "        example = json.loads(example)\n",
        "        \n",
        "        input_text = example['content']\n",
        "        ground_truth = example['groundTruth']\n",
        "        candidates = example['candidates']\n",
        "        \n",
        "        input_text = prompt(input_text, candidates, ground_truth)\n",
        "        input_texts.append(input_text)\n",
        "        labels.append('、'.join(ground_truth))\n",
        "\n",
        "    inputs = tokenizer(text=input_texts, return_token_type_ids=False)\n",
        "    labels = tokenizer(labels, return_token_type_ids=False)\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK4BVi3I0jhZ"
      },
      "outputs": [],
      "source": [
        "# def preprocess_data(data):\n",
        "#     input_texts = []\n",
        "#     labels = []\n",
        "\n",
        "#     for example in data:\n",
        "#         example = json.loads(example)\n",
        "        \n",
        "#         input_text = example['content']\n",
        "#         ground_truth = example['groundTruth']\n",
        "#         candidates = example['candidates']\n",
        "\n",
        "#         for i, idiom in enumerate(ground_truth):\n",
        "#             input_text = \"请从（）里选择出最合适的成语: \" + input_text\n",
        "#             candidates_str = '|'.join([c for c in candidates[i]])\n",
        "#             input_text = input_text.replace('#idiom#', \"（\" + candidates_str + \"）\", 1)\n",
        "\n",
        "#         input_texts.append(input_text)\n",
        "#         labels.append('、'.join(ground_truth))\n",
        "\n",
        "#     inputs = tokenizer(text=input_texts, return_token_type_ids=False)\n",
        "#     labels = tokenizer(labels, return_token_type_ids=False)\n",
        "#     return inputs, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o32SDRCOeSg"
      },
      "outputs": [],
      "source": [
        "# Load the Chinese Idioms dataset\n",
        "train_data_file = path+'data/train_15000.txt'\n",
        "val_data_file = path+'data/dev_2000.txt'\n",
        "\n",
        "\n",
        "\n",
        "with open(train_data_file, encoding='utf-8', errors='ignore') as f:\n",
        "    train_data = f.readlines()\n",
        "\n",
        "with open(val_data_file, encoding='utf-8', errors='ignore') as f:\n",
        "    val_data = f.readlines()\n",
        "\n",
        "train_inputs, train_labels = preprocess_data(train_data, prompt2)\n",
        "val_inputs, val_labels = preprocess_data(val_data, prompt2)\n",
        "\n",
        "# train_inputs = preprocess_data(train_data)\n",
        "# val_inputs = preprocess_data(val_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZePFJMnwZst8",
        "outputId": "e58500b9-d786-47b9-82f3-b12f8781e6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Encoding(num_tokens=151, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[12, 480, 9434, 57, 21, 25846, 291, 891, 4800, 1013, 2778, 146, 5684, 4800, 503, 237, 30846, 1954, 4800, 1676, 1313, 2903, 2330, 4800, 4453, 10765, 4800, 2050, 151, 36, 1744, 4800, 241, 38, 1546, 2862, 19, 367, 100, 134, 6064, 10581, 4034, 291, 3336, 20, 12, 3336, 10, 2936, 3441, 346, 991, 4115, 11793, 9, 5488, 24574, 94, 6, 1580, 1195, 31561, 5349, 6523, 145, 203, 364, 10, 222, 76, 1368, 9, 169, 123, 3132, 9, 70, 1335, 5488, 12963, 2847, 6, 107, 175, 683, 512, 3762, 38, 13028, 10, 5517, 9, 70, 8164, 7043, 10, 241, 362, 21258, 501, 4659, 43, 13758, 15779, 9, 295, 634, 43, 18, 23562, 1102, 2257, 9, 5406, 3733, 35, 7344, 27860, 5151, 6, 12549, 2810, 1193, 6014, 1003, 80, 22, 19188, 21236, 9, 14, 1010, 3145, 932, 596, 1359, 10328, 113, 2337, 2625, 27, 29527, 5635, 716, 1639, 5635, 1204, 69, 6, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['▁', '请', '依次', '从', '(', '超凡', '入', '圣', '|', '骨', '瘦', '如', '柴', '|', '青', '面', '獠', '牙', '|', '虎', '背', '熊', '腰', '|', '成人', '之美', '|', '肥', '头', '大', '耳', '|', '神', '不', '守', '舍', ')', '选择', '出', '最', '合适的', '成语', '填', '入', '_', ':', '▁', '_', '的', '掌', '柜', '只', '穿', '一件', '衬衫', ',', '坐在', '柜台', '里', '。', '几个', '堂', '倌', '穿着', '脏', '得', '发', '黑', '的', '白', '工作', '服', ',', '因为', '没有', '顾客', ',', '都', '散', '坐在', '桌子', '旁', '。', '这', '当', '儿', '看到', '这位', '不', '寻常', '的', '客人', ',', '都', '露出', '好奇', '的', '神', '色', '列宁', '曾', '批评', '他', '理论上', '的错误', ',', '同时', '认为', '他', '“', '所写的', '全部', '哲学', ',', '赶紧', '迎', '上', '前来', '伺', '候', '。', '聂', '赫', '留', '朵', '夫', '要', '了', '一瓶', '矿泉水', ',', '在', '离', '窗', '较', '远', '的地方', '挨', '着', '一张', '铺', '有', '肮脏', '桌', '布', '的小', '桌', '坐', '下', '。', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(len(train_inputs))\n",
        "print(train_inputs[0])\n",
        "print(train_inputs[0].ids)\n",
        "print(train_inputs[0].type_ids)\n",
        "print(train_inputs[0].tokens)\n",
        "\n",
        "# print(train_inputs[0].offsets)\n",
        "# print(train_inputs[0].attention_mask)\n",
        "# print(train_inputs[0].special_tokens_mask)\n",
        "# print(train_inputs[0].overflowing)\n",
        "\n",
        "# print(\"---------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thc3xa_tgwIo"
      },
      "outputs": [],
      "source": [
        "class IdiomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs['input_ids'][idx]\n",
        "        attention_mask = self.inputs['attention_mask'][idx]\n",
        "        # target_ids = self.inputs['input_ids'][idx]\n",
        "\n",
        "        target_ids = self.labels['input_ids'][idx]\n",
        "        target_attention_mask = self.labels['attention_mask'][idx]\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\":attention_mask, \"label_ids\":target_ids}\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_input = [torch.LongTensor(example['input_ids']) for example in batch]\n",
        "    batch_label = [torch.LongTensor(example['label_ids']) for example in batch]\n",
        "    batch_mask = [torch.LongTensor(example['attention_mask']) for example in batch]\n",
        "\n",
        "    padded_batch_input_ids = pad_sequence(batch_input, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    padded_batch_label = pad_sequence(batch_label, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    padded_batch_att_mask = pad_sequence(batch_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
        "\n",
        "def to_device(data, device):\n",
        "    new_data = {}\n",
        "    for k in data:\n",
        "        new_data[k] = data[k].to(device)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7OgKxH9g7d9"
      },
      "outputs": [],
      "source": [
        "train_dataset = IdiomDataset(train_inputs, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "val_dataset = IdiomDataset(val_inputs, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Hzdf4OPwtAs3"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykQVixLCcL7_"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module, train_loader:DataLoader, optimizer:optim.Optimizer, log_step=100):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    log_loss = 0.0\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        # print(\"idx:\", idx, \", log_step: \", log_step)\n",
        "        optimizer.zero_grad()\n",
        "        batch = to_device(batch, device)\n",
        "        loss = model(**batch).loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        log_loss += loss.item()\n",
        "\n",
        "        wandb.log({\"batch\": idx, \"train loss\": loss.item()})\n",
        "        wandb.log({\"batch\": idx, \"acc train loss\": log_loss})\n",
        "        \n",
        "        if (idx+1) % log_step == 0:\n",
        "            print(f\"Train Step: {idx} Loss: {log_loss / log_step}\")\n",
        "            log_loss = 0.0\n",
        "    return epoch_loss / len(train_loader)\n",
        "        \n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
        "    eval_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    print(\"eval_loader len:\", len(eval_loader))\n",
        "    for batch in eval_loader:\n",
        "        batch = to_device(batch, device)\n",
        "        output = model(**batch)\n",
        "        loss = output.loss\n",
        "        eval_loss += loss.item()\n",
        "        pred = output.logits.argmax(-1)\n",
        "        label = batch[\"labels\"]\n",
        "        correct += torch.where(label!=0, pred==label, 0).sum().item()\n",
        "        total += torch.sum(label!=0).item()\n",
        "\n",
        "    eval_acc = correct / total\n",
        "    eval_loss = eval_loss / len(eval_loader) \n",
        "    print(total, correct)\n",
        "    return eval_acc, eval_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8fccfeff40274dd9bfdbf455d4eafac9",
            "f6258c11973d4665b474dd1b395f3b75",
            "4030c1f963304388a35dbe11d02bbc74",
            "31cbda028718417fbba48e0ba1ba5abf",
            "513edb1560af44ea83606a1896b404f6",
            "c2b831aedf474481b5f4f9503ce4922d",
            "39972676008e40c2bccd8bdc1cda72bc",
            "56e54d8f8b0e4ba0888faca284e35d0c"
          ]
        },
        "id": "18gwTiRItAs3",
        "outputId": "978fe2b0-511d-42db-ee21-4524c046d1a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.20.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:1e6fmjuh) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fccfeff40274dd9bfdbf455d4eafac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">desert-deluge-3</strong> at: <a href='https://wandb.ai/zootopia/zootopia/runs/1e6fmjuh' target=\"_blank\">https://wandb.ai/zootopia/zootopia/runs/1e6fmjuh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230419_182657-1e6fmjuh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:1e6fmjuh). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230419_182856-cip1w14f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zootopia/zootopia/runs/cip1w14f' target=\"_blank\">clean-totem-4</a></strong> to <a href='https://wandb.ai/zootopia/zootopia' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/zootopia/zootopia' target=\"_blank\">https://wandb.ai/zootopia/zootopia</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/zootopia/zootopia/runs/cip1w14f' target=\"_blank\">https://wandb.ai/zootopia/zootopia/runs/cip1w14f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/zootopia/zootopia/runs/cip1w14f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb27a49d880>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "!pip3 install wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"zootopia\",\n",
        "    \n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "        \"epochs\": 5,\n",
        "        # \"learning_rate\": lr,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "0Pejvlkw_apv",
        "outputId": "7dc04d3f-3b06-4fba-d3f1-c9bf527b879b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 1\n",
            "Train Step: 0 Loss: 0.08606499671936035\n",
            "Train Step: 100 Loss: 4.718033996820449\n",
            "Train Step: 200 Loss: 1.2771234238147735\n",
            "Train Step: 300 Loss: 0.7394437858462334\n",
            "Train Step: 400 Loss: 0.6203460249304772\n",
            "Train Step: 500 Loss: 0.5692024856805802\n",
            "Train Step: 600 Loss: 0.5231441542506218\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a12fbb55ec01>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoches\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} Training Loss: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0meval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-7b7be54c994b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_step)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epoches = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, epoches+1):\n",
        "    print(f\"Training Epoch {epoch}\")\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    print(f\"Epoch {epoch} Training Loss: {train_loss}\")\n",
        "    eval_acc, eval_loss = evaluate(model, val_loader)\n",
        "    # wandb.log({\"epoch\": epoch, \"Eval Acc:\": eval_acc, \"Eval Loss:\": eval_loss})\n",
        "    print(f\"Epoch {epoch} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")\n",
        "    break\n",
        "wandb.finish()\n",
        "torch.save(model.state_dict(), path+\"T5_base_prompt2_ckpt.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "b72cN8KAtAs3"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9n3RHm0Pfbq"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def fill_idiom(model, loader):\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        batch = to_device(batch, device)\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, return_dict_in_generate=True, pad_token_id=0, max_length=512, top_k=15)\n",
        "        truncated_outputs = []\n",
        "\n",
        "        decode_texts = tokenizer.batch_decode([l[l != 0] for l in outputs[\"sequences\"]])\n",
        "        gold_texts = tokenizer.batch_decode([l[l != 0] for l in labels])\n",
        "\n",
        "        for gold, decode in zip(gold_texts, decode_texts):\n",
        "            l = set(gold.replace(\" \", \"\").replace(\"</s>\", \"\").split(\"、\"))\n",
        "            p = set(decode.replace(\" \", \"\").replace(\"</s>\", \"\").split(\"、\"))\n",
        "            all_labels.append(l)\n",
        "            all_preds.append(p)\n",
        "        # print(decode_texts)\n",
        "        # print(gold_texts)\n",
        "        # break\n",
        "    \n",
        "    return all_preds, all_labels\n",
        "\n",
        "def f1_score(sys, gold):\n",
        "    tp = 0\n",
        "    t = 0\n",
        "    p = 0\n",
        "    for s, g in zip(sys, gold):\n",
        "        t += len(g)\n",
        "        p += len(s)\n",
        "        tp += len(g & s)\n",
        "    precision = tp / p if p != 0 else 0\n",
        "    recall = tp / t if t != 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "    return precision, recall, f1, tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPNXqgJYjOG1",
        "outputId": "4aa4b6f1-0806-467a-a004-091076a6d015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Validation set is 0.5602851750891172\n",
            "Accuracy for Validation set is 1336\n"
          ]
        }
      ],
      "source": [
        "# validation set\n",
        "model.load_state_dict(torch.load(path+\"T5_base_prompt2_ckpt.pt\", map_location=device))\n",
        "sys, gold = fill_idiom(model, val_loader)\n",
        "p, r, f1, tp = f1_score(sys, gold)\n",
        "\n",
        "print(f\"Accuracy for Validation set is {f1}\")\n",
        "print(f\"Accuracy for Validation set is {tp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0GQJS-qtAs4"
      },
      "outputs": [],
      "source": [
        "# Load the Chinese Idioms dataset For Test set\n",
        "test_data_file = path+'data/test_2000.txt'\n",
        "\n",
        "with open(test_data_file, encoding='utf-8', errors='ignore') as f:\n",
        "    test_data = f.readlines()\n",
        "\n",
        "test_inputs, test_labels  = preprocess_data(test_data,prompt2)\n",
        "test_dataset = IdiomDataset(test_inputs, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ04JfLRtAs4",
        "outputId": "4382659c-d2f5-42b9-e87f-4b7d2dd17a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for Test set is 0.4539097266369994\n",
            "Accuracy for Test set is 1071\n"
          ]
        }
      ],
      "source": [
        "# test set\n",
        "model.load_state_dict(torch.load(path+\"T5_base_prompt2_ckpt.pt\", map_location=device))\n",
        "sys, gold = fill_idiom(model, test_loader)\n",
        "p, r, f1, tp = f1_score(sys, gold)\n",
        "\n",
        "print((f\"F1 score for Test set is {f1}\"))\n",
        "print(f\"Accuracy for Test set is {tp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UXK2woyTYbXH",
        "outputId": "09a06d01-be02-4d8b-cc4d-dc08ca1da52a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'嗤恶痛绝</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.decode([12]))\n",
        "tokenizer.decode([12, 31200, 2057, 1498, 1278, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ghchxOetAs4"
      },
      "source": [
        "# Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvQgN1cItAs4"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXKPoqbUtAs4",
        "outputId": "446f1660-0448-452f-9d57-b6fe43ed8b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\") \n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvGGLnEKtAs4"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5fe-JmVtAs4"
      },
      "outputs": [],
      "source": [
        "def prompt1(text, candidates, choice):\n",
        "    input_text = \"请从（）里选择出最合适的成语: \" + text\n",
        "    for i in range(len(choice)):\n",
        "        candidates_str = '|'.join([c for c in candidates[i]])\n",
        "        input_text = input_text.replace('#idiom#', \"（\" + candidates_str + \"）\", 1) \n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrxHrbsmtAs4"
      },
      "outputs": [],
      "source": [
        "def prompt2(text, candidates, choice):\n",
        "    input_text = \"请依次从（）选择出最合适的成语填入#idiom#: \" + text\n",
        "    for i in range(len(choice)):\n",
        "        candidates_str = '|'.join([c for c in candidates[i]])\n",
        "        input_text = input_text.replace('（）', \"（\" + candidates_str + \"）（）\", 1)\n",
        "    input_text = input_text.replace(\"（）\", \"\")\n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOqCsKmLtAs4"
      },
      "outputs": [],
      "source": [
        "def prompt3(text, candidates, choice):\n",
        "    ex = '''选择：[[“凭空捏造“, “高头大马“, “通力合作“, “同舟共济“, “和衷共济“, “蓬头垢面“, “紧锣密鼓“],[“叫苦连天“, “量体裁衣“, “金榜题名“, “百战不殆“, “知彼知己“, “有的放矢“, “风流才子“]]\\n输入：“世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得“，就需要各方面密切配合、#idiom#。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、#idiom#、灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。”\\n输出：通力合作, 有的放矢\\n'''\n",
        "    input_text = ex\n",
        "    input_text = input_text + f\"选择：{candidates}\\n输入：{text}\\n输出：\"\n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NI7Sy8LtAs4"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, prompt):\n",
        "    input_texts = []\n",
        "    labels = []\n",
        "\n",
        "    for example in data:\n",
        "        example = json.loads(example)\n",
        "        \n",
        "        input_text = example['content']\n",
        "        ground_truth = example['groundTruth']\n",
        "        candidates = example['candidates']\n",
        "        \n",
        "        input_text = prompt(input_text, candidates, ground_truth)\n",
        "        input_texts.append(input_text)\n",
        "        labels.append('、'.join(ground_truth))\n",
        "\n",
        "    inputs = tokenizer(text=input_texts, return_token_type_ids=False)\n",
        "    labels = tokenizer(labels, return_token_type_ids=False)\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFZBtx1BtAs4"
      },
      "outputs": [],
      "source": [
        "# Load the Chinese Idioms dataset For Test set\n",
        "test_data_file = path+'data/test_2000.txt'\n",
        "with open(test_data_file, encoding='utf-8', errors='ignore') as f:\n",
        "    test_data = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNg_BXnHtAs4"
      },
      "outputs": [],
      "source": [
        "test_inputs1, test_labels1 = preprocess_data(test_data, prompt1)\n",
        "test_dataset1 = IdiomDataset(test_inputs1, test_labels1)\n",
        "test_loader1 = DataLoader(test_dataset1, batch_size=8, collate_fn=collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pw_mCHMtAs4"
      },
      "outputs": [],
      "source": [
        "test_inputs2, test_labels2 = preprocess_data(test_data, prompt2)\n",
        "test_dataset2 = IdiomDataset(test_inputs2, test_labels2)\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=8, collate_fn=collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq1rLZ6ztAs4"
      },
      "outputs": [],
      "source": [
        "test_inputs3, test_labels3  = preprocess_data(test_data, prompt3)\n",
        "test_dataset3 = IdiomDataset(test_inputs3, test_labels3)\n",
        "test_loader3 = DataLoader(test_dataset3, batch_size=8, collate_fn=collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mZEAPJ1tAs5",
        "outputId": "36f4a00d-925c-414e-9463-4f4ca3eaa4e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '请', '依次', '从', '(', '旷', '日', '持久', '|', '公正', '廉洁', '|', '苦', '口', '婆', '心', '|', '现身', '说法', '|', '白', '日', '做梦', '|', '深入浅出', '|', '肺', '腑', '之', '言', ')', '选择', '出', '最', '合适的', '成语', '填', '入', '#', 'id', 'io', 'm', '#', ':', '▁', '只要', '路过', '的', '旅客', '稍有', '迟', '疑', ',', '或者', '对他们', '的宣传', '单', '多', '看', '几', '眼', ',', '基本上', '这个', '旅客', '就别', '想', '轻松', '脱', '身', '了', ',', '记者', '就在', '9', '月', '3', '日', '接', '站', '时', '目睹', '了', '这样', '一幕', ':', '一个', '学生', '接', '过', '招生', '人员', '递', '来', '的宣传', '单', ',', '只是', '问', '了一下', '“', '你们', '学校', '有没有', '分数', '要求', '?”', '两个', '招生', '人员', '就', '“', '白话', '”', '开了', ',', '一个', '表示', '分数', '都', '好', '说', ',', '只要有', '好', '学', '的精神', ';', '另一个', '则', '#', 'id', 'io', 'm', '#', ',', '大', '讲', '自己', '选择', '的专业', '现在', '收获', '颇', '丰', ';', '最后', '在', '招生', '人员', '“', '我们学校', '毕业后', '可以', '完全', '解决', '就业', '”', '的', '忽悠', '下', ',', '这个', '学生', '旅客', '被', '他们', '拉', '上了', '到', '校', '参观', '的', '班车', '。', '</s>']\n",
            "['▁', '选择', ':', '[', '[', '“', '凭', '空', '捏', '造', '“', ',', '▁“', '高', '头', '大', '马', '“', ',', '▁“', '通', '力', '合作', '“', ',', '▁“', '同', '舟', '共', '济', '“', ',', '▁“', '和', '衷', '共', '济', '“', ',', '▁“', '蓬', '头', '垢', '面', '“', ',', '▁“', '紧', '锣', '密', '鼓', '“', ']', ',', '[', '“', '叫', '苦', '连', '天', '“', ',', '▁“', '量', '体裁', '衣', '“', ',', '▁“', '金', '榜', '题', '名', '“', ',', '▁“', '百', '战', '不', '殆', '“', ',', '▁“', '知', '彼', '知己', '“', ',', '▁“', '有', '的', '放', '矢', '“', ',', '▁“', '风流', '才', '子', '“', ']', ']', '▁', '输入', ':“', '世锦赛', '的整体', '水平', '远高于', '亚洲', '杯', ',', '要', '如同', '亚洲', '杯', '那样', '“', '鱼', '与', '熊', '掌', '兼', '得', '“', ',', '就需要', '各方面', '密切', '配合', '、', '#', 'id', 'io', 'm', '#', '。', '作为', '主帅', '的', '俞', '觉', '敏', ',', '除了', '得', '打破', '保守', '思想', ',', '敢于', '破', '格', '用', '人', ',', '还得', '巧', '于', '用', '兵', '、', '#', 'id', 'io', 'm', '#', '、', '灵活', '排', '阵', ',', '指挥', '得', '当', ',', '力争', '通过', '比赛', '推', '新人', '、', '出', '佳绩', '、', '出', '新的', '战斗力', '。', '”', '▁', '输出', ':', '通', '力', '合作', ',', '▁', '有', '的', '放', '矢', '▁', '选择', ':', '[', '[', \"'\", '旷', '日', '持久', \"'\", ',', '▁', \"'\", '公正', '廉洁', \"'\", ',', '▁', \"'\", '苦', '口', '婆', '心', \"'\", ',', '▁', \"'\", '现身', '说法', \"'\", ',', '▁', \"'\", '白', '日', '做梦', \"'\", ',', '▁', \"'\", '深入浅出', \"'\", ',', '▁', \"'\", '肺', '腑', '之', '言', \"'\", ']', ']', '▁', '输入', ':', '只要', '路过', '的', '旅客', '稍有', '迟', '疑', ',', '或者', '对他们', '的宣传', '单', '多', '看', '几', '眼', ',', '基本上', '这个', '旅客', '就别', '想', '轻松', '脱', '身', '了', ',', '记者', '就在', '9', '月', '3', '日', '接', '站', '时', '目睹', '了', '这样', '一幕', ':', '一个', '学生', '接', '过', '招生', '人员', '递', '来', '的宣传', '单', ',', '只是', '问', '了一下', '“', '你们', '学校', '有没有', '分数', '要求', '?”', '两个', '招生', '人员', '就', '“', '白话', '”', '开了', ',', '一个', '表示', '分数', '都', '好', '说', ',', '只要有', '好', '学', '的精神', ';', '另一个', '则', '#', 'id', 'io', 'm', '#', ',', '大', '讲', '自己', '选择', '的专业', '现在', '收获', '颇', '丰', ';', '最后', '在', '招生', '人员', '“', '我们学校', '毕业后', '可以', '完全', '解决', '就业', '”', '的', '忽悠', '下', ',', '这个', '学生', '旅客', '被', '他们', '拉', '上了', '到', '校', '参观', '的', '班车', '。', '▁', '输出', ':', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(test_inputs2[0].tokens)\n",
        "print(test_inputs3[0].tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use the pre-trianed model"
      ],
      "metadata": {
        "id": "aJIg8MLMjxBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaGNNvQctAs5",
        "outputId": "a3c8d087-b772-4ff2-9d8e-7bb00d4ccd3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for Test set is 0.03008336353751359\n",
            "Accuracy for Test set is 83\n",
            "F1 score for Test set is 0.08479448117275079\n",
            "Accuracy for Test set is 295\n",
            "F1 score for Test set is 0\n",
            "Accuracy for Test set is 0\n"
          ]
        }
      ],
      "source": [
        "# prompt engineering for test set (prompt1)\n",
        "# model.load_state_dict(torch.load(path+\"T5_base_prompt2_ckpt.pt.pt\", map_location=device))\n",
        "sys1, gold1 = fill_idiom(model, test_loader1)\n",
        "p1, r1, f11, tp1 = f1_score(sys1, gold1)\n",
        "print((f\"F1 score for Test set is {f11}\"))\n",
        "print(f\"Accuracy for Test set is {tp1}\")\n",
        "\n",
        "# prompt engineering for test set (prompt2)\n",
        "sys2, gold2 = fill_idiom(model, test_loader2)\n",
        "p2, r2, f12, tp2 = f1_score(sys2, gold2)\n",
        "print((f\"F1 score for Test set is {f12}\"))\n",
        "print(f\"Accuracy for Test set is {tp2}\")\n",
        "\n",
        "# prompt engineering for test set (prompt3)\n",
        "sys3, gold3 = fill_idiom(model, test_loader3)\n",
        "p3, r3, f13, tp3 = f1_score(sys3, gold3)\n",
        "print((f\"F1 score for Test set is {f13}\"))\n",
        "print(f\"Accuracy for Test set is {tp3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use the fine-tune model"
      ],
      "metadata": {
        "id": "i-_fhOzYjqdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(path+\"T5_base_prompt2_ckpt.pt\", map_location=device))\n",
        "# prompt engineering for test set (prompt1)\n",
        "sys1, gold1 = fill_idiom(model, test_loader1)\n",
        "p1, r1, f11, tp1 = f1_score(sys1, gold1)\n",
        "print((f\"F1 score for Test set is {f11}\"))\n",
        "print(f\"Accuracy for Test set is {tp1}\")\n",
        "\n",
        "# prompt engineering for test set (prompt2)\n",
        "sys2, gold2 = fill_idiom(model, test_loader2)\n",
        "p2, r2, f12, tp2 = f1_score(sys2, gold2)\n",
        "print((f\"F1 score for Test set is {f12}\"))\n",
        "print(f\"Accuracy for Test set is {tp2}\")\n",
        "\n",
        "# prompt engineering for test set (prompt3)\n",
        "sys3, gold3 = fill_idiom(model, test_loader3)\n",
        "p3, r3, f13, tp3 = f1_score(sys3, gold3)\n",
        "print((f\"F1 score for Test set is {f13}\"))\n",
        "print(f\"Accuracy for Test set is {tp3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aT42xGYjm0D",
        "outputId": "13136c62-25c4-4da2-ac8a-ec1b657c804d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for Test set is 0.3311442786069652\n",
            "Accuracy for Test set is 832\n",
            "F1 score for Test set is 0.4539097266369994\n",
            "Accuracy for Test set is 1071\n",
            "F1 score for Test set is 0.0013761467889908258\n",
            "Accuracy for Test set is 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "a7KOJWufxTjf",
        "outputId": "06b42b9b-33d0-48bb-cb0b-9324bef4844e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型生成: 在亚洲杯上，中国选手俞觉敏的表现非常出色。她不仅发挥了出色的表现力和团队合作能力外也取得了很多成功：通过比赛推新人、出佳绩以及新战斗力等各方面的配合来达到自己的目的并取得胜利;同时她也能够与其他国家的比赛中保持联系并与对手进行交流并获得经验教训等等都是她的成就之一。\n",
            "****************************************************************************************************\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
        "# import torch\n",
        "# from torch import cuda\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"mxmax/Chinese_Chat_T5_Base\") \n",
        "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "# model.to(device)\n",
        "def postprocess(text):\n",
        "    return text.replace(\".\", \"\").replace('</>','')\n",
        "\n",
        "def answer_fn(text, top_k=50):\n",
        "    encoding = tokenizer(text=[text], truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(device) \n",
        "    out = model.generate(**encoding, return_dict_in_generate=True, output_scores=False, max_length=512,temperature=0.5,do_sample=True,repetition_penalty=3.0 ,top_k=top_k)\n",
        "    result = tokenizer.batch_decode(out[\"sequences\"], skip_special_tokens=True)\n",
        "    return postprocess(result[0])\n",
        "\n",
        "x1 = \"\"\"世锦赛的整体水平远高于亚洲杯，要如同亚洲杯那样“鱼与熊掌兼得”，就需要各方面密切配合、（凭空捏造|高头大马|通力合作|同舟共济|和衷共济|蓬头垢面|紧锣密鼓）。作为主帅的俞觉敏，除了得打破保守思想，敢于破格用人，还得巧于用兵、(叫苦连天|量体裁衣|金榜题名|百战不殆|知彼知己|风流才子)、\n",
        "灵活排阵，指挥得当，力争通过比赛推新人、出佳绩、出新的战斗力。\"\"\"\n",
        "\n",
        "# y1 = [\"高头大马\", \"叫苦连天\"]\n",
        "\n",
        "result=answer_fn(x1, top_k=50)\n",
        "print(\"模型生成:\",result)\n",
        "print('*'*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXDm-C9g2Ivd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fccfeff40274dd9bfdbf455d4eafac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6258c11973d4665b474dd1b395f3b75",
              "IPY_MODEL_4030c1f963304388a35dbe11d02bbc74"
            ],
            "layout": "IPY_MODEL_31cbda028718417fbba48e0ba1ba5abf"
          }
        },
        "f6258c11973d4665b474dd1b395f3b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513edb1560af44ea83606a1896b404f6",
            "placeholder": "​",
            "style": "IPY_MODEL_c2b831aedf474481b5f4f9503ce4922d",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "4030c1f963304388a35dbe11d02bbc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39972676008e40c2bccd8bdc1cda72bc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56e54d8f8b0e4ba0888faca284e35d0c",
            "value": 1
          }
        },
        "31cbda028718417fbba48e0ba1ba5abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513edb1560af44ea83606a1896b404f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b831aedf474481b5f4f9503ce4922d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39972676008e40c2bccd8bdc1cda72bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e54d8f8b0e4ba0888faca284e35d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}