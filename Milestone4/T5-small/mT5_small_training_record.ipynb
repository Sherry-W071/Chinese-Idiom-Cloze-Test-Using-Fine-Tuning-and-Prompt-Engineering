{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931cb5cd-10af-445e-b38c-389bb5d8837d",
   "metadata": {
    "id": "931cb5cd-10af-445e-b38c-389bb5d8837d"
   },
   "source": [
    "# mT5-small Fine-tuning (Record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1WDTm1VFloi",
   "metadata": {
    "id": "q1WDTm1VFloi"
   },
   "source": [
    "#### **The model checkpoint can be downloaded from: [Google Drive](https://drive.google.com/file/d/1uuevTEvrMhLiW4cfijcMkpfz3IrdwgG-/view?usp=share_link)*\n",
    "#### **This note book is NOT the final one but for record only. The data is slightly changed in the final notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f1500-424a-44af-991a-ab96caf01238",
   "metadata": {
    "id": "338f1500-424a-44af-991a-ab96caf01238"
   },
   "source": [
    "## Imports, Device Setting and Weight and Bias Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db85f5a-7e83-4f2e-9e5b-783132661495",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2db85f5a-7e83-4f2e-9e5b-783132661495",
    "outputId": "a30cf16a-8f93-4d13-c02e-e598705c8a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.15.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip3 install wandb\n",
    "! pip install sentencepiece\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import cuda, nn, optim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11dcc013-acec-4506-b1a4-a17912c5d13f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11dcc013-acec-4506-b1a4-a17912c5d13f",
    "outputId": "1350d288-7183-48ce-862f-2e8fd1a0a6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "manual_seed = 585\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gfGHEqITuwMD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "gfGHEqITuwMD",
    "outputId": "b1c28892-dfd7-44d1-ab4f-0034150210f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqmygrace\u001b[0m (\u001b[33mzoooootopia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqmygrace\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20230421_013455-tzlg37u0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qmygrace/Zootopia/runs/tzlg37u0' target=\"_blank\">bright-durian-11</a></strong> to <a href='https://wandb.ai/qmygrace/Zootopia' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qmygrace/Zootopia' target=\"_blank\">https://wandb.ai/qmygrace/Zootopia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qmygrace/Zootopia/runs/tzlg37u0' target=\"_blank\">https://wandb.ai/qmygrace/Zootopia/runs/tzlg37u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/qmygrace/Zootopia/runs/tzlg37u0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f40a07c39d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"Zootopia\", entity=\"qmygrace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6y1YrsaF7HY",
   "metadata": {
    "id": "o6y1YrsaF7HY"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c2f97e-7c04-4abc-b23a-8e4a29ec7ffc",
   "metadata": {
    "id": "f7c2f97e-7c04-4abc-b23a-8e4a29ec7ffc"
   },
   "source": [
    "## Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ae48ef-34a0-44fd-8489-0d4b13dc73d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99ae48ef-34a0-44fd-8489-0d4b13dc73d0",
    "outputId": "d7112686-6148-4622-dadf-a319df2ac4b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/google/mt5-small\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1f4b3-2f98-4704-bff4-12f1f9ad055a",
   "metadata": {
    "id": "2bf1f4b3-2f98-4704-bff4-12f1f9ad055a"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0aa74f8-a3fa-4746-a720-c831ac6ef645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0aa74f8-a3fa-4746-a720-c831ac6ef645",
    "outputId": "dfb85bd9-bdc2-4702-f9e3-da826489baf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"groundTruth\": [\"发扬光大\", \"平易近人\", \"温文尔雅\"], \"candidates\": [[\"意气风发\", \"街谈巷议\", \"人才辈出\", \"一脉相传\", \"后继有人\", \"发扬光大\", \"腥风血雨\"], [\"平易近人\", \"落落大方\", \"八仙过海\", \"彬彬有礼\", \"史无前例\", \"盛气凌人\", \"好自为之\"], [\"不拘小节\", \"风流潇洒\", \"无病呻吟\", \"言谈举止\", \"壮志凌云\", \"关门闭户\", \"温文尔雅\"]], \"content\": \"由实力派演员刘威饰演的清华第三任校长蒋南翔，是我国著名的青年运动家和教育家，他跟清华终身校长梅贻琦一样，都是由清华人自己培养出来的校长。历史上的蒋南翔是著名的“一二九”学生救亡运动的领导人之一，他在清华校长之位14年期间，不但很好的继承了清华建校之初的优秀传统与理念，而且更加的#idiom#，他把清华的教师队伍扩大了将近5倍，将清华本科人数破万，为新中国培养了大量的有用人才。在《天行健》中饰演蒋南翔的刘威是观众所熟悉的著名实力派演员，早在1987年刘威就在《关东大侠》中饰演豪爽仗义的关云天一角而获得了金鸡奖最佳男主角的提名，后来更是因在《唐明皇》中精湛的表演而一举夺得金鹰奖最佳男演员奖。此次《天行健》选定刘威来出演正是看中了他#idiom#的表演方式和对人物深入内心的刻画。至此，《天行健》中涉及的三位清华校长的人选都已经曝光，#idiom#的第一任校长赵文?、稳重坚毅的第二任校长孙逊、亲切务实的第三任校长刘威，再加上梁思成、林徽因、朱自清、闻一多等一批“大师”的加盟，相信作为清华百年校庆重点项目之一的《天行健》一定会带领观众重温那段不能抹去的历史。\", \"realCount\": 3}\\n', '{\"groundTruth\": [\"肥头大耳\"], \"candidates\": [[\"超凡入圣\", \"骨瘦如柴\", \"青面獠牙\", \"虎背熊腰\", \"成人之美\", \"肥头大耳\", \"神不守舍\"]], \"content\": \"#idiom#的掌柜只穿一件衬衫，坐在柜台里。几个堂倌穿着脏得发黑的白工作服，因为没有顾客，都散坐在桌子旁。这当儿看到这位不寻常的客人，都露出好奇的神色列宁曾批评他理论上的错误，同时认为他“所写的全部哲学，赶紧迎上前来伺候。聂赫留朵夫要了一瓶矿泉水，在离窗较远的地方挨着一张铺有肮脏桌布的小桌坐下。\", \"realCount\": 1}\\n'] \n",
      " 20000 2500 2500\n"
     ]
    }
   ],
   "source": [
    "# path = '../data/'    # change the path as needed\n",
    "path = '/content/gdrive/My Drive/585data/'\n",
    "\n",
    "def read_data(file):\n",
    "    with open (path+file) as t:\n",
    "        data = t.readlines()\n",
    "    return data\n",
    "\n",
    "train_set = read_data('train_data.txt')[:20000]\n",
    "dev_set = read_data('dev_data.txt')[:2500]\n",
    "test_set = read_data('test_data.txt')[:2500]\n",
    "\n",
    "# type(train_set)\n",
    "print(train_set[:2], '\\n', len(train_set), len(dev_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dZ1JICTnZPma",
   "metadata": {
    "id": "dZ1JICTnZPma"
   },
   "outputs": [],
   "source": [
    "# preprocess_idx = -1\n",
    "# def replace(match):\n",
    "#     global preprocess_idx\n",
    "#     preprocess_idx += 1\n",
    "#     return 'extra {}'.format(preprocess_idx)\n",
    "\n",
    "# text = '由实力派演员刘威饰演的清华第三任校长蒋南翔，是我国著名的青年运动家和教育家，他跟清华终身校长梅贻琦一样，都是由清华人自己培养出来的校长。历史上的蒋南翔是著名的“一二九”学生救亡运动的领导人之一，他在清华校长之位14年期间，不但很好的继承了清华建校之初的优秀传统与理念，而且更加的#idiom#，他把清华的教师队伍扩大了将近5倍，将清华本科人数破万，为新中国培养了大量的有用人才。在《天行健》中饰演蒋南翔的刘威是观众所熟悉的著名实力派演员，早在1987年刘威就在《关东大侠》中饰演豪爽仗义的关云天一角而获得了金鸡奖最佳男主角的提名，后来更是因在《唐明皇》中精湛的表演而一举夺得金鹰奖最佳男演员奖。此次《天行健》选定刘威来出演正是看中了他#idiom#的表演方式和对人物深入内心的刻画。至此，《天行健》中涉及的三位清华校长的人选都已经曝光，#idiom#的第一任校长赵文?、稳重坚毅的第二任校长孙逊、亲切务实的第三任校长刘威，再加上梁思成、林徽因、朱自清、闻一多等一批“大师”的加盟，相信作为清华百年校庆重点项目之一的《天行健》一定会带领观众重温那段不能抹去的历史。'\n",
    "# re.sub(r'#idiom#', replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3965b6-eaff-42ff-ac70-dac21b8dbd21",
   "metadata": {
    "id": "9c3965b6-eaff-42ff-ac70-dac21b8dbd21"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    text_input = []\n",
    "    idiom_output = []\n",
    "    for i in range(len(data)):\n",
    "        data[i] = eval(data[i])\n",
    "        input_text = data[i]['content']\n",
    "        ground_truth = data[i]['groundTruth']\n",
    "        candidates = data[i]['candidates']\n",
    "\n",
    "        candidate_str = ''\n",
    "        for candidate in candidates:\n",
    "            candidate_str += '('+'|'.join(candidate)+')'\n",
    "        \n",
    "        preprocess_idx = -1\n",
    "        def replace(match):\n",
    "            nonlocal preprocess_idx\n",
    "            preprocess_idx += 1\n",
    "            return 'extra{}'.format(preprocess_idx)\n",
    "        input_text = re.sub(r'#idiom#', replace, input_text)\n",
    "\n",
    "        instruction = '请从下列括号中分别选择合适的成语填入空缺处：{}'.format(candidate_str)\n",
    "        # input_text = input_text.replace('#idiom#', '_')\n",
    "        output_text = ','.join(ground_truth)\n",
    "        \n",
    "        text_input.append(instruction+'\\n'+input_text)\n",
    "        idiom_output.append(output_text)\n",
    "    \n",
    "    print(text_input[0], idiom_output[0])    \n",
    "    input_tok = tokenizer.batch_encode_plus(text_input,\n",
    "                                            add_special_tokens=False, \n",
    "                                            return_token_type_ids=False)\n",
    "    output_tok = tokenizer.batch_encode_plus(idiom_output, \n",
    "                                             add_special_tokens=False,\n",
    "                                             return_token_type_ids=False)\n",
    "    return input_tok, output_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad9389a-0f2a-4222-8f67-e9f968e6c734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ad9389a-0f2a-4222-8f67-e9f968e6c734",
    "outputId": "08dc7451-6f9e-47b8-f280-cd75de079009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请从下列括号中分别选择合适的成语填入空缺处：(意气风发|街谈巷议|人才辈出|一脉相传|后继有人|发扬光大|腥风血雨)(平易近人|落落大方|八仙过海|彬彬有礼|史无前例|盛气凌人|好自为之)(不拘小节|风流潇洒|无病呻吟|言谈举止|壮志凌云|关门闭户|温文尔雅)\n",
      "由实力派演员刘威饰演的清华第三任校长蒋南翔，是我国著名的青年运动家和教育家，他跟清华终身校长梅贻琦一样，都是由清华人自己培养出来的校长。历史上的蒋南翔是著名的“一二九”学生救亡运动的领导人之一，他在清华校长之位14年期间，不但很好的继承了清华建校之初的优秀传统与理念，而且更加的extra0，他把清华的教师队伍扩大了将近5倍，将清华本科人数破万，为新中国培养了大量的有用人才。在《天行健》中饰演蒋南翔的刘威是观众所熟悉的著名实力派演员，早在1987年刘威就在《关东大侠》中饰演豪爽仗义的关云天一角而获得了金鸡奖最佳男主角的提名，后来更是因在《唐明皇》中精湛的表演而一举夺得金鹰奖最佳男演员奖。此次《天行健》选定刘威来出演正是看中了他extra1的表演方式和对人物深入内心的刻画。至此，《天行健》中涉及的三位清华校长的人选都已经曝光，extra2的第一任校长赵文?、稳重坚毅的第二任校长孙逊、亲切务实的第三任校长刘威，再加上梁思成、林徽因、朱自清、闻一多等一批“大师”的加盟，相信作为清华百年校庆重点项目之一的《天行健》一定会带领观众重温那段不能抹去的历史。 发扬光大,平易近人,温文尔雅\n",
      "请从下列括号中分别选择合适的成语填入空缺处：(深恶痛绝|人人自危|恨入骨髓|不胜枚举|嗤之以鼻|走马看花|不屑一顾)\n",
      "另据了解，北京一个对垃圾短信extra0的老人，利用该软件总共呼死了近2000个号码。20分钟呼上万号码记者昨天在百度里输入“呼死你软件”，出现了7000多个相关网页，随机登录几个网站，发现软件均需花钱购买，价格从200元至500元不等。 深恶痛绝\n",
      "请从下列括号中分别选择合适的成语填入空缺处：(旷日持久|公正廉洁|苦口婆心|现身说法|白日做梦|深入浅出|肺腑之言)\n",
      "只要路过的旅客稍有迟疑，或者对他们的宣传单多看几眼，基本上这个旅客就别想轻松脱身了，记者就在9月3日接站时目睹了这样一幕：一个学生接过招生人员递来的宣传单，只是问了一下“你们学校有没有分数要求？”两个招生人员就“白话”开了，一个表示分数都好说，只要有好学的精神；另一个则extra0，大讲自己选择的专业现在收获颇丰；最后在招生人员“我们学校毕业后可以完全解决就业”的忽悠下，这个学生旅客被他们拉上了到校参观的班车。 现身说法\n"
     ]
    }
   ],
   "source": [
    "train_input, train_output = preprocess(train_set)\n",
    "dev_input, dev_output = preprocess(dev_set)\n",
    "test_input, test_output = preprocess(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbb4375-3f3f-441c-81b4-be13b24160bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dbb4375-3f3f-441c-81b4-be13b24160bb",
    "outputId": "a864459f-04d8-4082-e56b-cf63e6ccbc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask']) dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.keys(), train_output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dab48c9-e312-4706-b5fa-d408bc8d227a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dab48c9-e312-4706-b5fa-d408bc8d227a",
    "outputId": "cd5a37bb-f717-4fcc-cc18-65bae9109ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259, 20256, 5229, 2446, 19783, 149527, 3688, 1223, 91486, 29133, 5072, 92396, 493, 4449, 20139, 82635, 4484, 8123, 47193, 13746, 267, 312, 10691, 11755, 8893, 5685, 409, 16628, 30014, 158795, 26896, 409, 47145, 130509, 2371, 409, 1374, 117723, 6497, 19946, 409, 3592, 88623, 64947, 409, 5685, 54029, 4491, 1146, 409, 239243, 8893, 18643, 16189, 4829, 5064, 18272, 8659, 1193, 409, 12265, 12265, 1146, 4222, 409, 7704, 39098, 6994, 4093, 409, 204126, 204126, 1637, 21158, 409, 22695, 5941, 2884, 13733, 409, 19510, 11755, 91708, 1193, 409, 3586, 5081, 2037, 2904, 4829, 1597, 104151, 2144, 16984, 409, 8893, 8041, 239151, 185028, 409, 5941, 14469, 242248, 172615, 409, 9812, 30014, 49425, 32401, 409, 78590, 16706, 91708, 9896, 409, 14428, 8394, 80855, 27841, 409, 17794, 4565, 8216, 24582, 271, 259, 10135, 77006, 19700, 138014, 18538, 18157, 73502, 20936, 493, 9060, 8423, 42258, 12307, 155410, 128057, 4938, 93781, 261, 1543, 44635, 237766, 44283, 61449, 3203, 1107, 9716, 3203, 261, 3763, 18475, 9060, 8423, 52095, 7431, 155410, 29702, 242959, 216892, 62685, 261, 24134, 10135, 9060, 141789, 9512, 106277, 162480, 155410, 306, 218000, 493, 128057, 4938, 93781, 1543, 237766, 591, 1374, 3178, 13591, 365, 18105, 23616, 65201, 61449, 493, 181313, 51838, 261, 175021, 9060, 8423, 155410, 2904, 5396, 1635, 848, 59632, 261, 196603, 199006, 218824, 1322, 9060, 8423, 10389, 14470, 2904, 12869, 493, 94962, 67714, 2645, 92113, 261, 39722, 56424, 493, 51056, 460, 261, 3763, 9803, 9060, 8423, 493, 70015, 125219, 103247, 1322, 3661, 8659, 428, 9697, 261, 3661, 9060, 8423, 140344, 69951, 18540, 2029, 261, 2037, 2188, 3161, 106277, 1322, 207010, 180469, 47145, 306, 1083, 2518, 2910, 3541, 39450, 2640, 1223, 73502, 20936, 128057, 4938, 93781, 493, 18538, 18157, 1543, 114912, 4398, 167296, 493, 144362, 77006, 19700, 138014, 261, 7321, 1083, 56875, 848, 18538, 18157, 125159, 2518, 14428, 8854, 1146, 97451, 2640, 1223, 73502, 20936, 29789, 73891, 185422, 27056, 493, 14428, 9896, 2910, 1374, 15347, 4925, 221896, 2445, 42651, 28251, 41844, 11323, 187777, 493, 17459, 3094, 261, 116536, 104763, 16563, 1083, 2518, 29572, 5853, 29145, 2640, 1223, 12348, 190829, 493, 130822, 4925, 1374, 49425, 75076, 5880, 2445, 110632, 28251, 41844, 11323, 138014, 28251, 306, 77554, 2518, 2910, 3541, 39450, 2640, 7213, 5160, 18538, 18157, 3480, 74089, 131967, 4833, 1223, 218782, 51056, 353, 493, 130822, 22492, 1107, 2991, 58839, 62375, 190351, 493, 35487, 15828, 306, 9903, 14781, 62784, 2910, 3541, 39450, 2640, 1223, 107572, 493, 2092, 5396, 9060, 8423, 155410, 28936, 7213, 4794, 17909, 117471, 261, 51056, 338, 179657, 12307, 155410, 42759, 4565, 291, 292, 34224, 5742, 74688, 126732, 493, 26454, 12307, 155410, 38157, 168402, 292, 25187, 13802, 237751, 493, 42258, 12307, 155410, 18538, 18157, 261, 7519, 158919, 44586, 15011, 4449, 292, 6892, 195057, 16563, 292, 53380, 5081, 9060, 292, 53982, 1374, 3139, 2395, 136507, 591, 49194, 365, 493, 6690, 261, 90759, 24673, 9060, 8423, 156542, 14470, 39583, 39961, 15268, 51838, 493, 2518, 2910, 3541, 39450, 2640, 231061, 182699, 114912, 5742, 17794, 10776, 14219, 23808, 126767, 6072, 158335, 306] \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_input['input_ids'][0], '\\n', train_input['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ItvsDz6wgf2D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItvsDz6wgf2D",
    "outputId": "15665c8a-4fbc-44f1-92e8-01bee63d2323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259, 5685, 54029, 4491, 1146, 261, 5064, 18272, 8659, 1193, 261, 17794, 4565, 8216, 24582] \n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_output['input_ids'][0], '\\n', train_output['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745d083f-5ef4-4ef2-95ae-88edc375a5aa",
   "metadata": {
    "id": "745d083f-5ef4-4ef2-95ae-88edc375a5aa"
   },
   "outputs": [],
   "source": [
    "class IdiomDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "\n",
    "        target_ids = self.outputs['input_ids'][idx]\n",
    "        target_attention_mask = self.outputs['attention_mask'][idx]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\":attention_mask, \"output_ids\":target_ids}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_input = [torch.LongTensor(example['input_ids']) for example in batch]\n",
    "    batch_output = [torch.LongTensor(example['output_ids']) for example in batch]\n",
    "    batch_mask = [torch.LongTensor(example['attention_mask']) for example in batch]\n",
    "\n",
    "    padded_batch_input_ids = pad_sequence(batch_input, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_label = pad_sequence(batch_output, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_batch_att_mask = pad_sequence(batch_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\"input_ids\": padded_batch_input_ids, \"attention_mask\": padded_batch_att_mask, \"labels\": padded_batch_label}\n",
    "\n",
    "def to_device(data, device):\n",
    "    new_data = {}\n",
    "    for k in data:\n",
    "        # k = k.to(device)\n",
    "        new_data[k] = data[k].to(device)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2306fb26-febb-4159-b957-89944b73a9c4",
   "metadata": {
    "id": "2306fb26-febb-4159-b957-89944b73a9c4"
   },
   "outputs": [],
   "source": [
    "train_dataset = IdiomDataset(train_input, train_output)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "dev_dataset = IdiomDataset(dev_input, dev_output)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fc45d-c94f-4dcc-a845-285e1959ef6e",
   "metadata": {
    "id": "3f4fc45d-c94f-4dcc-a845-285e1959ef6e"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e09128-fe25-44b1-8572-496a460bf870",
   "metadata": {
    "id": "20e09128-fe25-44b1-8572-496a460bf870"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module, eval_loader:DataLoader):\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    print(\"eval_loader len:\", len(eval_loader))\n",
    "    for batch in eval_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        eval_loss += loss.item()\n",
    "        pred = output.logits.argmax(-1)\n",
    "        label = batch[\"labels\"]\n",
    "        correct += torch.where(label!=0, pred==label, 0).sum().item()\n",
    "        total += torch.sum(label!=0).item()\n",
    "\n",
    "    eval_acc = correct / total\n",
    "    eval_loss = eval_loss / len(eval_loader) \n",
    "    print(total, correct)\n",
    "    return eval_acc, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4aed717-87ff-4abf-a24a-c3397bfec0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4aed717-87ff-4abf-a24a-c3397bfec0f5",
    "outputId": "c446e93f-3617-4f38-e91e-5a6d18e10e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0 Loss: 0.3624109268188477\n",
      "Train Step: 100 Loss: 27.885444202423095\n",
      "Train Step: 200 Loss: 20.266760244369507\n",
      "Train Step: 300 Loss: 16.646393766403197\n",
      "Train Step: 400 Loss: 14.34015974998474\n",
      "Train Step: 500 Loss: 12.613938312530518\n",
      "Train Step: 600 Loss: 10.975036611557007\n",
      "Train Step: 700 Loss: 10.338030376434325\n",
      "Train Step: 800 Loss: 9.289741277694702\n",
      "Train Step: 900 Loss: 8.410621366500855\n",
      "Train Step: 1000 Loss: 7.64582968711853\n",
      "Train Step: 1100 Loss: 6.7045935535430905\n",
      "Train Step: 1200 Loss: 5.868394136428833\n",
      "Train Step: 1300 Loss: 5.172049405574799\n",
      "Train Step: 1400 Loss: 4.590031695365906\n",
      "Train Step: 1500 Loss: 4.2740762996673585\n",
      "Train Step: 1600 Loss: 3.9429748368263247\n",
      "Train Step: 1700 Loss: 3.383105719089508\n",
      "Train Step: 1800 Loss: 3.169616365432739\n",
      "Train Step: 1900 Loss: 3.0035462272167206\n",
      "Train Step: 2000 Loss: 2.489843978881836\n",
      "Train Step: 2100 Loss: 2.2652867710590363\n",
      "Train Step: 2200 Loss: 2.05966717004776\n",
      "Train Step: 2300 Loss: 1.8886240816116333\n",
      "Train Step: 2400 Loss: 1.7699570846557617\n",
      "Epoch: 1 Loss is: 19104.092910170555\n",
      "eval_loader len: 313\n",
      "14842 12000\n",
      "Epoch 1 Eval Acc: 0.8085163724565423; Eval Loss: 0.5788515556734591\n",
      "Train Step: 0 Loss: 0.007011102437973022\n",
      "Train Step: 100 Loss: 0.555310134589672\n",
      "Train Step: 200 Loss: 0.5110707502067089\n",
      "Train Step: 300 Loss: 0.4977967768907547\n",
      "Train Step: 400 Loss: 0.4778363764286041\n",
      "Train Step: 500 Loss: 0.47456473112106323\n",
      "Train Step: 600 Loss: 0.4641479080915451\n",
      "Train Step: 700 Loss: 0.4675141669809818\n",
      "Train Step: 800 Loss: 0.4533259654045105\n",
      "Train Step: 900 Loss: 0.4464793656766415\n",
      "Train Step: 1000 Loss: 0.44608487993478774\n",
      "Train Step: 1100 Loss: 0.4253613889217377\n",
      "Train Step: 1200 Loss: 0.4259974279999733\n",
      "Train Step: 1300 Loss: 0.4299730043113232\n",
      "Train Step: 1400 Loss: 0.4283511213958263\n",
      "Train Step: 1500 Loss: 0.4055023731291294\n",
      "Train Step: 1600 Loss: 0.40270177498459814\n",
      "Train Step: 1700 Loss: 0.4176038579642773\n",
      "Train Step: 1800 Loss: 0.3884035040438175\n",
      "Train Step: 1900 Loss: 0.3943640618026257\n",
      "Train Step: 2000 Loss: 0.405628357976675\n",
      "Train Step: 2100 Loss: 0.3922239625453949\n",
      "Train Step: 2200 Loss: 0.3811748513579369\n",
      "Train Step: 2300 Loss: 0.38964151322841645\n",
      "Train Step: 2400 Loss: 0.3776901787519455\n",
      "Epoch: 2 Loss is: 1084.0170597434044\n",
      "eval_loader len: 313\n",
      "14842 12495\n",
      "Epoch 2 Eval Acc: 0.8418676728203746; Eval Loss: 0.3910989172447223\n",
      "Train Step: 0 Loss: 0.004478715360164642\n",
      "Train Step: 100 Loss: 0.3726155632734299\n",
      "Train Step: 200 Loss: 0.3474459046125412\n",
      "Train Step: 300 Loss: 0.3394324915111065\n",
      "Train Step: 400 Loss: 0.3373596797883511\n",
      "Train Step: 500 Loss: 0.3499791416525841\n",
      "Train Step: 600 Loss: 0.33229412257671354\n",
      "Train Step: 700 Loss: 0.3436283869296312\n",
      "Train Step: 800 Loss: 0.3298389072716236\n",
      "Train Step: 900 Loss: 0.30883948907256126\n",
      "Train Step: 1000 Loss: 0.3425845791399479\n",
      "Train Step: 1100 Loss: 0.32188831239938737\n",
      "Train Step: 1200 Loss: 0.3223768511414528\n",
      "Train Step: 1300 Loss: 0.3196987025439739\n",
      "Train Step: 1400 Loss: 0.30834932684898375\n",
      "Train Step: 1500 Loss: 0.3342520214617252\n",
      "Train Step: 1600 Loss: 0.31005698345601557\n",
      "Train Step: 1700 Loss: 0.33018073439598083\n",
      "Train Step: 1800 Loss: 0.30185060173273087\n",
      "Train Step: 1900 Loss: 0.3173738460242748\n",
      "Train Step: 2000 Loss: 0.33115080997347834\n",
      "Train Step: 2100 Loss: 0.31351400636136534\n",
      "Train Step: 2200 Loss: 0.322204210460186\n",
      "Train Step: 2300 Loss: 0.3129747214913368\n",
      "Train Step: 2400 Loss: 0.3267287227511406\n",
      "Epoch: 3 Loss is: 817.6416110619903\n",
      "eval_loader len: 313\n",
      "14842 12741\n",
      "Epoch 3 Eval Acc: 0.8584422584557337; Eval Loss: 0.3207328821095034\n",
      "Train Step: 0 Loss: 0.002855781614780426\n",
      "Train Step: 100 Loss: 0.24936757996678352\n",
      "Train Step: 200 Loss: 0.26169325038790703\n",
      "Train Step: 300 Loss: 0.2615162750333548\n",
      "Train Step: 400 Loss: 0.2575240971893072\n",
      "Train Step: 500 Loss: 0.27279280982911586\n",
      "Train Step: 600 Loss: 0.24726648308336735\n",
      "Train Step: 700 Loss: 0.25553674161434176\n",
      "Train Step: 800 Loss: 0.24695243008434772\n",
      "Train Step: 900 Loss: 0.25272507302463054\n",
      "Train Step: 1000 Loss: 0.2555119474977255\n",
      "Train Step: 1100 Loss: 0.2633194899559021\n",
      "Train Step: 1200 Loss: 0.2712740097939968\n",
      "Train Step: 1300 Loss: 0.26569498181343076\n",
      "Train Step: 1400 Loss: 0.2572473981231451\n",
      "Train Step: 1500 Loss: 0.246702236905694\n",
      "Train Step: 1600 Loss: 0.24065378241240978\n",
      "Train Step: 1700 Loss: 0.2540111781656742\n",
      "Train Step: 1800 Loss: 0.2409694716334343\n",
      "Train Step: 1900 Loss: 0.2547171653062105\n",
      "Train Step: 2000 Loss: 0.2419091631472111\n",
      "Train Step: 2100 Loss: 0.25697039879858496\n",
      "Train Step: 2200 Loss: 0.24719532653689386\n",
      "Train Step: 2300 Loss: 0.23700149342417717\n",
      "Train Step: 2400 Loss: 0.23512297458946704\n",
      "Epoch: 4 Loss is: 632.8200289681554\n",
      "eval_loader len: 313\n",
      "14842 12979\n",
      "Epoch 4 Eval Acc: 0.8744778331761218; Eval Loss: 0.2809618115663148\n",
      "Train Step: 0 Loss: 0.0022598305344581605\n",
      "Train Step: 100 Loss: 0.1787763412296772\n",
      "Train Step: 200 Loss: 0.17557175159454347\n",
      "Train Step: 300 Loss: 0.1930907702073455\n",
      "Train Step: 400 Loss: 0.1933220148459077\n",
      "Train Step: 500 Loss: 0.19899711765348913\n",
      "Train Step: 600 Loss: 0.18597319096326828\n",
      "Train Step: 700 Loss: 0.18956306107342244\n",
      "Train Step: 800 Loss: 0.19231182366609573\n",
      "Train Step: 900 Loss: 0.20125974223017692\n",
      "Train Step: 1000 Loss: 0.18427588189020752\n",
      "Train Step: 1100 Loss: 0.1759845021367073\n",
      "Train Step: 1200 Loss: 0.16901450838893653\n",
      "Train Step: 1300 Loss: 0.1663634507358074\n",
      "Train Step: 1400 Loss: 0.17619289480149747\n",
      "Train Step: 1500 Loss: 0.16598187167197465\n",
      "Train Step: 1600 Loss: 0.17204287316650152\n",
      "Train Step: 1700 Loss: 0.1782617973536253\n",
      "Train Step: 1800 Loss: 0.1796263836324215\n",
      "Train Step: 1900 Loss: 0.17243560660630464\n",
      "Train Step: 2000 Loss: 0.20568211145699025\n",
      "Train Step: 2100 Loss: 0.18708712719380854\n",
      "Train Step: 2200 Loss: 0.1707417617738247\n",
      "Train Step: 2300 Loss: 0.17130329988896847\n",
      "Train Step: 2400 Loss: 0.19398574620485307\n",
      "Epoch: 5 Loss is: 456.0205141287297\n",
      "eval_loader len: 313\n",
      "14842 13075\n",
      "Epoch 5 Eval Acc: 0.8809459641557742; Eval Loss: 0.2713608031455701\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "epoches = 5       \n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epoches):\n",
    "    epoch_loss = 0.0\n",
    "    log_loss = 0.0\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        batch = to_device(batch, device)\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        log_loss += loss.item()\n",
    "\n",
    "        wandb.log({'batch':idx, 'train_loss': loss.item()})\n",
    "        wandb.log({'batch':idx, 'accumulated_train_loss_in_this_1k_batches': log_loss})\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Train Step: {idx} Loss: {log_loss / 100}\")\n",
    "            log_loss = 0.0\n",
    "    print(f\"Epoch: {epoch+1} Loss is: {epoch_loss}\")\n",
    "    eval_acc, eval_loss = evaluate(model, dev_loader)\n",
    "    print(f\"Epoch {epoch+1} Eval Acc: {eval_acc}; Eval Loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b27ea9-82a6-429a-9198-da7e64683bba",
   "metadata": {
    "id": "27b27ea9-82a6-429a-9198-da7e64683bba"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path+\"mT5-small_model_5epoches.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "QU3_bzhwBXud",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU3_bzhwBXud",
    "outputId": "13a12564-0d66-4538-d496-9a5110e608ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 300,176,768 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nExNVXOrwSe_",
   "metadata": {
    "id": "nExNVXOrwSe_"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "DLFW9KkpjPQ9",
   "metadata": {
    "id": "DLFW9KkpjPQ9"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_idiom(model, loader):\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        batch = to_device(batch, device)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = model.generate(input_ids=input_ids, \n",
    "                                 attention_mask=attention_mask, \n",
    "                                 return_dict_in_generate=True, \n",
    "                                 pad_token_id=tokenizer.pad_token_id, \n",
    "                                 max_length=512, \n",
    "                                 top_k=15)\n",
    "        truncated_outputs = []\n",
    "\n",
    "        decode_texts = tokenizer.batch_decode([l[l != 0] for l in outputs['sequences']])\n",
    "        gold_texts = tokenizer.batch_decode([l[l != 0] for l in labels])\n",
    "        # print(decode_texts, gold_texts)\n",
    "        for gold, decode in zip(gold_texts, decode_texts):\n",
    "            l = set(gold.replace(' ', '').replace('[CLS]', '').split(','))\n",
    "            p = set(decode.replace(' ', '').replace('[CLS]', '').split(','))\n",
    "            # print(l, p)\n",
    "            all_labels.append(l)\n",
    "            all_preds.append(p)\n",
    "        # print(decode_texts)\n",
    "        # print(gold_texts)\n",
    "        # break\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def f1_score(sys, gold):\n",
    "    tp = 0\n",
    "    total = 0\n",
    "    pos = 0\n",
    "    for s, g in zip(sys, gold):\n",
    "        total += len(g)\n",
    "        pos += len(s)\n",
    "        tp += len(g & s)\n",
    "    precision = tp / pos if pos != 0 else 0\n",
    "    recall = tp / total if total != 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return precision, recall, f1, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ZiMhMM_ZjWsj",
   "metadata": {
    "id": "ZiMhMM_ZjWsj"
   },
   "outputs": [],
   "source": [
    "sys, gold = fill_idiom(model, dev_loader)\n",
    "p, r, f1, tp = f1_score(sys, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "TvhKNUaaOh3j",
   "metadata": {
    "id": "TvhKNUaaOh3j"
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for s, g in zip(sys, gold):\n",
    "    total += len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "NVeTyoI4MDRF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVeTyoI4MDRF",
    "outputId": "84a2bbaf-6a3b-4556-999d-9307c08f8ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate amount for Validation set is 1314 out of 3058\n",
      "Accuracy for Validation set is 0.4296926095487247\n",
      "F1 score for Validation set is 0.4300441826215022\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accurate amount for Validation set is {tp} out of {total}\")\n",
    "print(f\"Accuracy for Validation set is {tp/total}\")\n",
    "print(f\"F1 score for Validation set is {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gMSWr-DEtcEB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMSWr-DEtcEB",
    "outputId": "e52c0a87-4e56-43ce-bf86-9ee99aef60cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'深恶痛绝'},\n",
       " {'井井有条'},\n",
       " {'跃跃欲试'},\n",
       " {'无与伦比'},\n",
       " {'一语道破', '不胜其烦', '评头品足'},\n",
       " {'千篇一律'},\n",
       " {'罪魁祸首'},\n",
       " {'聪明才智'},\n",
       " {'千载难逢'},\n",
       " {'苦中作乐'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "YulPihgI0l98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YulPihgI0l98",
    "outputId": "dca3b972-71ba-4d53-dde3-68d7a6bfd8e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'深恶痛绝'},\n",
       " {'杂乱无章'},\n",
       " {'磨刀霍霍'},\n",
       " {'独一无二'},\n",
       " {'一语道破', '不厌其烦', '品头题足'},\n",
       " {'大同小异'},\n",
       " {'罪魁祸首'},\n",
       " {'聪明才智'},\n",
       " {'千载难逢'},\n",
       " {'酸甜苦辣'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bftVEK_Wc4uM",
   "metadata": {
    "id": "bftVEK_Wc4uM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
